{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9685426,"sourceType":"datasetVersion","datasetId":5920638}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"#INSTALL LIBRARIES------------------------------------------\n!pip install transformers scikit-learn datasets wandb  torch_geometric word2number \n#!pip uninstall -y nltk\n# !pip install --upgrade preprocessing\n\n# !pip install nltk==3.2.4\n\n#!pip install -U augmentation\n# !pip install --upgrade ipykernel\n# !git clone https://github.com/monteleone-1883922/hw2_nlp.git\n# %cd hw2_nlp/\n# !git checkout huggingFaceBase\n# %cd ..\n# !cp hw2_nlp/manipulations.py .\n# !cp hw2_nlp/augmentation.py .\n# !rm -r hw2_nlp/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHARZQ6fIXm8","outputId":"74c29994-b200-45d1-c125-8dff210a5f15","execution":{"iopub.status.busy":"2024-10-21T16:46:06.331905Z","iopub.execute_input":"2024-10-21T16:46:06.332877Z","iopub.status.idle":"2024-10-21T16:46:23.902891Z","shell.execute_reply.started":"2024-10-21T16:46:06.332832Z","shell.execute_reply":"2024-10-21T16:46:23.901721Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting word2number\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: word2number\n  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=02542f622415c93d5992f962ace59c14354c878a8db0268181a4a0f227ebd246\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built word2number\nInstalling collected packages: word2number, torch_geometric\nSuccessfully installed torch_geometric-2.6.1 word2number-1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#IMPORTS-----------------------------\nfrom pprint import pprint\nfrom datasets import load_dataset\nfrom transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom torch.nn import Linear, ReLU\nimport pdb\nimport numpy as np, torch, random as rnd, torch.nn as nn, wandb\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nimport sys, os, json\nfrom transformers import AutoModelForQuestionAnswering\nfrom torch.nn.functional import cosine_similarity\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score,  ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer\nimport os, time\n#from augmentation import augment_data, augment_data_multithread\nimport random\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom torch_geometric.nn import GATv2Conv\nfrom torch_geometric.nn.norm import LayerNorm\nimport torch.nn.init as init\n","metadata":{"id":"aZ4RHIcXDNcA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba8c57ee-e0ce-436a-f150-df5810c28889","execution":{"iopub.status.busy":"2024-10-21T16:46:23.905044Z","iopub.execute_input":"2024-10-21T16:46:23.905411Z","iopub.status.idle":"2024-10-21T16:46:33.955155Z","shell.execute_reply.started":"2024-10-21T16:46:23.905373Z","shell.execute_reply":"2024-10-21T16:46:33.954205Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# adversarial = load_dataset(\"iperbole/adversarial_fever_nli\")[\"test\"]\n\n# ds = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n\n# training_set = ds[\"train\"]\n\n# validation_set = ds[\"validation\"]\n\n# test_set = ds[\"test\"]\n\n\n","metadata":{"id":"initial_id","execution":{"iopub.status.busy":"2024-10-21T16:46:33.956405Z","iopub.execute_input":"2024-10-21T16:46:33.956790Z","iopub.status.idle":"2024-10-21T16:46:33.961233Z","shell.execute_reply.started":"2024-10-21T16:46:33.956756Z","shell.execute_reply":"2024-10-21T16:46:33.960266Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# random.seed(42)\n\n# training_set, new_data, info_augmentations = augment_data(training_set, 2000)","metadata":{"id":"pSp_pqX7wy4_","execution":{"iopub.status.busy":"2024-10-21T16:46:33.964035Z","iopub.execute_input":"2024-10-21T16:46:33.964855Z","iopub.status.idle":"2024-10-21T16:46:34.005016Z","shell.execute_reply.started":"2024-10-21T16:46:33.964816Z","shell.execute_reply":"2024-10-21T16:46:34.003612Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# grouped_data = {}\n# for sample in new_data:\n#     augment_value = sample['augment_method']\n#     grouped_data[augment_value] = grouped_data.get(augment_value, []) + [sample]\n# pprint(info_augmentations)\n\n# total_attempts = 0\n# total_successes = 0\n\n# # Itera attraverso il dizionario e somma i valori\n# for key, values in info_augmentations.items():\n#     total_attempts += values['count']\n#     total_successes += values['success']\n\n# # Stampa i risultati\n# print(f\"Totale tentativi: {total_attempts}\")\n# print(f\"Totale successi: {total_successes}\")","metadata":{"id":"adl5-XZtM30z","execution":{"iopub.status.busy":"2024-10-21T16:46:34.007285Z","iopub.execute_input":"2024-10-21T16:46:34.007704Z","iopub.status.idle":"2024-10-21T16:46:34.016513Z","shell.execute_reply.started":"2024-10-21T16:46:34.007663Z","shell.execute_reply":"2024-10-21T16:46:34.015442Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# pprint(grouped_data['CHANGE_NUMBERS'][3])\n\n# NEGATE_PART_PREMISE = 1\n#     # anything -> same thing\n#     SYNONYM = 2\n#     # anything -> negation\n#     ANTINOMY_PART_PREMISE = 3\n#     # anything -> same thing\n#     HYPONYM_PREMISE = 4\n#     # anything -> neutral\n#     SWITCH_DATA = 5\n#     # anything -> neutral\n#     SWITCH_PARTIAL_DATA = 6\n#     # anything -> entailment\n#     TAKE_PART_PREMISE = 7\n#     # entailment/negation -> opposite\n#     NEGATE_HYPOTHESIS = 8\n#     # anything -> same thing\n#     HYPERNYM_HYPOTHESIS = 9\n#     # anything -> negation\n#     IMPOSSIBILITY = 10\n#     # entailment -> entailment\n#     TRUNCATE_HYPOTHESIS = 11\n#     # anything -> entailment\n#     TAUTOLOGY = 12\n#     # anything -> entailment\n#     DUPLICATE_HYPOTHESIS = 13\n#     # entailment -> negation/entailment\n#     CHANGE_NUMBERS = 14\n","metadata":{"id":"Pg7h8xTxwoud","execution":{"iopub.status.busy":"2024-10-21T16:46:34.017588Z","iopub.execute_input":"2024-10-21T16:46:34.018649Z","iopub.status.idle":"2024-10-21T16:46:34.031191Z","shell.execute_reply.started":"2024-10-21T16:46:34.018603Z","shell.execute_reply":"2024-10-21T16:46:34.030105Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# print(training_set[0]['hypothesis'])\n# print(training_set[0]['premise'])\n# print(training_set[1]['hypothesis'])\n# print(training_set[1]['premise'])\n# print(training_set[2]['hypothesis'])\n# print(training_set[2]['premise'])","metadata":{"id":"xTXYcNXftIjd","execution":{"iopub.status.busy":"2024-10-21T16:46:34.032791Z","iopub.execute_input":"2024-10-21T16:46:34.033489Z","iopub.status.idle":"2024-10-21T16:46:34.039978Z","shell.execute_reply.started":"2024-10-21T16:46:34.033435Z","shell.execute_reply":"2024-10-21T16:46:34.038602Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# import nltk\n# from nltk.corpus import stopwords\n\n# nltk.download('stopwords')\n# print(stopwords.words('english'))\n\n","metadata":{"id":"l6yGkP9JJwmn","execution":{"iopub.status.busy":"2024-10-21T16:46:34.041613Z","iopub.execute_input":"2024-10-21T16:46:34.042002Z","iopub.status.idle":"2024-10-21T16:46:34.048506Z","shell.execute_reply.started":"2024-10-21T16:46:34.041967Z","shell.execute_reply":"2024-10-21T16:46:34.047241Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# f1 = \"test per vedere come va\"\n# f2 = \"questa è una prova\"\n\n# tokenized = tokenizer(f1+ tokenizer.eos_token + f2, return_tensors='pt', padding='max_length', max_length=40, return_token_type_ids=True)\n\n# print(tokenized)\n# print(tokenized[\"input_ids\"].shape)\n# print(type(tokenized))\n\n# out = model(**tokenized)\n# print(out['last_hidden_state'].shape)\n\n# print(out.last_hidden_state.mean(dim=-1).squeeze().shape)","metadata":{"id":"izN_nvlqSoUg","execution":{"iopub.status.busy":"2024-10-21T16:46:34.050018Z","iopub.execute_input":"2024-10-21T16:46:34.050372Z","iopub.status.idle":"2024-10-21T16:46:34.060599Z","shell.execute_reply.started":"2024-10-21T16:46:34.050339Z","shell.execute_reply":"2024-10-21T16:46:34.059518Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Function to print a progress bar\ndef print_progress_bar(percentuale: float, lunghezza_barra: int = 30, text: str=\"\") -> None:\n    blocchi_compilati = int(lunghezza_barra * percentuale)\n    barra = \"[\" + \"=\" * (blocchi_compilati - 1) + \">\" + \" \" * (lunghezza_barra - blocchi_compilati) + \"]\"\n    sys.stdout.write(f\"\\r{barra} {percentuale * 100:.2f}% complete \" + text)\n    sys.stdout.flush()","metadata":{"id":"yUEyqUjxq8tC","execution":{"iopub.status.busy":"2024-10-21T16:46:34.065804Z","iopub.execute_input":"2024-10-21T16:46:34.067864Z","iopub.status.idle":"2024-10-21T16:46:34.076115Z","shell.execute_reply.started":"2024-10-21T16:46:34.067779Z","shell.execute_reply":"2024-10-21T16:46:34.074849Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# from nltk.corpus import wordnet as wn\n# from nltk.tokenize import word_tokenize\n\n# nltk.download('wordnet')\n# nltk.download('punkt')\n\n\n# class NLIDataset(Dataset):\n\n#     def __init__(self, data, file_name='',load = False, adversarial=False, do_remove_stopwords=False,\n#                  do_remove_punctuation=False, do_use_similarities=False, base_set=True, do_lemmatization=False):\n#         self.sentence_info = None\n#         self.labels = None\n#         self.sentences = None\n#         self.load = load\n#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n#         self.encode_labels = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n#         self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n#         self.model = RobertaModel.from_pretrained('roberta-base').to(self.device)\n#         self.distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n#         self.file_name = file_name\n#         self.do_use_similarities = do_use_similarities\n#         self.do_lemmatization = do_lemmatization\n#         self.do_remove_punctuation = do_remove_punctuation\n#         self.base_set = base_set\n#         self.do_remove_stopwords = do_remove_stopwords\n#         self.adversarial = adversarial\n#         self.organize_data(data)\n#         self.tokenizer = None\n#         self.model = None\n#         self.count = 0\n\n#     def organize_data(self, data):\n#         info_samples = []\n#         sentences = []\n#         labels = []\n#         premise_hypotesis = []\n#         if not self.load or not os.path.isfile(self.file_name):\n#             for num_sample, sample in enumerate(data):\n#                 print_progress_bar(num_sample / len(data), text=\" | loading data\")\n#                 #breakpoint()\n#                 labels.append(self.encode_labels[sample[\"label\"]])\n#                 premise_hypotesis.append(sample['premise'] + \" [SEP] \" + sample['hypothesis'])\n#                 sentence = []\n#                 list_sentence_premise = [token[\"rawText\"] for token in sample['srl']['premise']['tokens']]\n#                 list_sentence_hypothesis = [token[\"rawText\"] for token in sample['srl']['hypothesis']['tokens']]\n#                 info_sample = {\"ids_verbs\": [], \"edges\": []}\n#                 idx = 0\n#                 edge_idx = 0\n#                 verb_edge_idx = -1\n#                 repeated_nodes = {}\n#                 # premise -----------------------------\n#                 for annotation in sample['srl']['premise']['annotations']:\n#                     if verb_edge_idx != -1:\n#                         info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n#                         info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n#                     verb_edge_idx = edge_idx\n#                     edge_idx += 1\n#                     verb_word_only = True\n#                     if len(sample['wsd']['premise']) > annotation['tokenIndex'] and sample['wsd']['premise'][annotation['tokenIndex']]['nltkSynset'] != 'O' and sample['wsd']['premise'][annotation['tokenIndex']]['pos'] == 'VERB':\n#                         examples = wn.synset(sample['wsd']['premise'][annotation['tokenIndex']]['nltkSynset']).examples()\n#                         id_verb = []\n#                         for example in examples:\n#                             if example != '':\n#                                 id_verb.append(idx)\n#                                 idx += 1\n#                                 verb_word_only = False\n#                                 sentence.append( example + ' [SEP] ')\n#                         info_sample[\"ids_verbs\"].append(tuple(id_verb))\n#                     if verb_word_only:\n#                         info_sample[\"ids_verbs\"].append((idx,))\n#                         idx += 1\n#                         sentence.append(list_sentence_premise[annotation['tokenIndex']] + ' [SEP] ')\n#                     for element in annotation['verbatlas']['roles']:\n#                         if element['span']:\n#                             if (element['span'][0], element['span'][1]) in repeated_nodes:\n#                                 info_sample[\"edges\"].append((repeated_nodes[(element['span'][0], element['span'][1])], verb_edge_idx))\n#                                 info_sample[\"edges\"].append((verb_edge_idx, repeated_nodes[(element['span'][0], element['span'][1])]))\n#                             else:\n#                                 info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n#                                 info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n#                                 edge_idx += 1\n#                                 idx += 1\n#                                 sentence.append(' '.join(list_sentence_premise[element['span'][0]:element['span'][1]]) + ' [SEP] ')\n#                                 repeated_nodes[(element['span'][0], element['span'][1])] = edge_idx\n#                 repeated_nodes.clear()\n#                 verb_edge_idx = -1\n#                 # hypothesis -----------------------------\n#                 for annotation in sample['srl']['hypothesis']['annotations']:\n#                     if verb_edge_idx != -1:\n#                         info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n#                         info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n#                     verb_edge_idx = edge_idx\n#                     edge_idx += 1\n#                     verb_word_only = True\n#                     if len(sample['wsd']['hypothesis']) > annotation['tokenIndex'] and sample['wsd']['hypothesis'][annotation['tokenIndex']]['nltkSynset'] != 'O' and sample['wsd']['hypothesis'][annotation['tokenIndex']]['pos'] == 'VERB':\n#                         examples = wn.synset(sample['wsd']['hypothesis'][annotation['tokenIndex']]['nltkSynset']).examples()\n#                         id_verb = []\n#                         for example in examples:\n#                             if example != '':\n#                                 id_verb.append(idx)\n#                                 idx += 1\n#                                 verb_word_only = False\n#                                 sentence.append(example + ' [SEP] ')\n#                         info_sample[\"ids_verbs\"].append(tuple(id_verb))\n#                     if verb_word_only:\n#                         info_sample[\"ids_verbs\"].append((idx,))\n#                         idx += 1\n#                         sentence.append(list_sentence_hypothesis[annotation['tokenIndex']] + ' [SEP] ')\n#                     for element in annotation['verbatlas']['roles']:\n#                         if element['span']:\n#                             if (element['span'][0], element['span'][1]) in repeated_nodes:\n#                                 info_sample[\"edges\"].append(\n#                                     (repeated_nodes[(element['span'][0], element['span'][1])], verb_edge_idx))\n#                                 info_sample[\"edges\"].append(\n#                                     (verb_edge_idx, repeated_nodes[(element['span'][0], element['span'][1])]))\n#                             else:\n#                                 info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n#                                 info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n#                                 edge_idx += 1\n#                                 idx += 1\n#                                 sentence.append(' '.join(list_sentence_premise[element['span'][0]:element['span'][1]]) + ' [SEP] ')\n#                                 repeated_nodes[(element['span'][0], element['span'][1])] = edge_idx\n#                 info_sample[\"num_nodes\"] = edge_idx\n#                 info_sample[\"num_sentences\"] = idx\n#                 sentences.append(' '.join(sentence))\n#                 info_samples.append(info_sample)\n#             if self.file_name != '':\n#                 with open(self.file_name, 'w') as file:\n#                     json.dump({'sentences': sentences, 'info_samples': info_samples, 'labels': labels, 'premise_hypotesis': premise_hypotesis}, file)\n#         else:\n#             with open(self.file_name, 'r') as file:\n#                 loaded_data = json.load(file)\n#             sentences = loaded_data['sentences']\n#             info_samples = loaded_data['info_samples']\n#             labels = loaded_data['labels']\n#             premise_hypotesis = loaded_data['premise_hypotesis']\n\n#         self.sentences = sentences\n#         self.sentence_info = info_samples\n#         self.labels = labels\n#         self.premise_hypotesis = premise_hypotesis\n\n#     def __len__(self):\n#         return len(self.labels)\n\n#     def __getitem__(self, idx):\n#         return self.sentences[idx], self.labels[idx], self.sentence_info[idx], self.premise_hypotesis[idx]\n\n#     def get_deleted_per_epoch(self, num_epochs):\n#         out = self.count / num_epochs\n#         self.count = 0\n#         return out\n\n#     def collate(self, batch):\n#         #breakpoint()\n#         inputs = []\n#         gold_outputs = []\n#         edges = []\n#         verb_ids = []\n#         offset_edges = 0\n#         offset_sentences = 0\n#         num_nodes = []\n#         classic_inputs = []\n#         for input_batch, gold_outputs_batch, sample_info_batch, premise_hypotesis in batch:\n#             if len(word_tokenize(input_batch)) < 500:\n#                 inputs.append(input_batch)\n#                 gold_outputs.append(gold_outputs_batch)\n#                 classic_inputs.append(premise_hypotesis)\n#                 edges.extend([(x + offset_edges, y + offset_edges) for x, y in sample_info_batch[\"edges\"]])\n\n#                 # Convertiamo da tuple a liste\n\n#                 offset_edges += sample_info_batch[\"num_nodes\"]\n\n\n#                 verb_ids.append(sample_info_batch[\"ids_verbs\"])\n#                 offset_sentences += sample_info_batch[\"num_sentences\"]\n#             else:\n#                 self.count += 1\n#         #breakpoint()\n#         inputs = self.distilbert_tokenizer(\n#             inputs,\n#             max_length=512,\n#             return_offsets_mapping=True,\n#             padding=\"max_length\",\n#             return_tensors=\"pt\"\n#         )\n#         classic_inputs = self.distilbert_tokenizer(\n#             classic_inputs,\n#             max_length=512,\n#             return_offsets_mapping=True,\n#             padding=\"max_length\",\n#             return_tensors=\"pt\"\n#         )\n\n\n#         gold_outputs = torch.tensor(gold_outputs, dtype=torch.long)\n#         edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n#         return inputs['input_ids'].to(self.device), inputs['attention_mask'].to(self.device), gold_outputs.to(self.device), \\\n#             edges.to(self.device), verb_ids, classic_inputs.to(self.device)\n\n\n\n\n\n\n#     def get_dataloader(self, batch_size):\n#         return DataLoader(self, batch_size=batch_size, shuffle=True, collate_fn=self.collate)\n\n\n# # prompt: create a torch model using roberta and a linear layer\n\n# class RobertaClassifier(nn.Module):\n#     def __init__(self, use_similarity, num_labels=3, dropout=0):\n#         super(RobertaClassifier, self).__init__()\n#         self.distilbert = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n#         self.dropout = nn.Dropout(dropout)\n#         self.gnn = GATv2Conv(self.distilbert.config.hidden_size, 64, dropout=0.3)\n#         # self.gnn1 = GATConv(64, 64)\n#         self.gnn2 = GATv2Conv(64, 32, dropout=0.3)  # ,heads=2)\n#         self.linear = nn.Linear(self.distilbert.config.hidden_size + 32, 64)\n#         #self.linear2 = nn.Linear(128, 128)\n#         self.classifier = nn.Linear(64, 3)\n#         self.initialize_weights()\n#         self.relu = nn.LeakyReLU()\n#         self.batchnorm_linear = nn.LayerNorm(64)\n#         self.batchnorm_gnn = LayerNorm(64)  # Modifica con hidden_size corretto\n#         self.batchnorm_gnn2 = LayerNorm(32)\n#         self.special_token = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\").sep_token_id\n\n#     def initialize_weights(self):\n#         for layer in self.children():\n#             if isinstance(layer, GATv2Conv):\n#                 # Inizializzazione dei pesi per GATConv\n#                 init.kaiming_normal_(layer.lin_l.weight, a=0.01)\n\n#                 if layer.lin_l.bias is not None:\n#                     init.zeros_(layer.lin_l.bias)\n\n\n#                 # Se hai anche la parte destra del layer di attenzione\n#                 if hasattr(layer, 'lin_r') and layer.lin_r is not None:\n#                     init.kaiming_normal_(layer.lin_r.weight, a=0.01)  # destra\n#                     if layer.lin_r.bias is not None:\n#                         init.zeros_(layer.lin_r.bias)\n\n#             elif isinstance(layer, nn.Linear):\n#                 # Inizializzazione Xavier per i layer lineari\n#                 init.kaiming_normal_(layer.weight, a=0.01)\n\n#                 if layer.bias is not None:\n#                     init.zeros_(layer.bias)\n\n#     # def extract_next_verb(verb_ids, verb_idx, k, range_sep):\n#     #     new_k = -1\n#     #     for i in range(verb_idx, len(verb_ids)):\n#     #         out_of_range = False\n#     #         for j in range(k, len(verb_ids[verb_idx])):\n#     #             if verb_ids[i][j].item() >= range_sep:\n#     #                 out_of_range = True\n#     #                 new_k = j\n#     #                 break\n#     #         k = 0\n#     #         if not out_of_range:\n#     #             return i, new_k, verb_ids[i][0].item()\n#     #     return -1,-1,-1\n\n\n#     def freeze(self, epoch):\n#         if epoch == 1:\n#             freeze_until_layer = 99999\n#         else:\n#             freeze_until_layer = None  # Non congela nulla\n\n#         # Congela i layer di DistilBERT\n#         if freeze_until_layer is not None:\n#             for idx, param in enumerate(self.distilbert.parameters()):\n#                 if idx < freeze_until_layer:\n#                     param.requires_grad = False\n#                 else:\n#                     param.requires_grad = True\n\n#     def forward(self, input_ids, attention_mask, edges, verb_ids, classic_inputs):\n#         #breakpoint()\n#         inputs = torch.cat((input_ids, classic_inputs['input_ids']), dim=0)\n#         attention_mask = torch.cat((attention_mask, classic_inputs['attention_mask']), dim=0)\n\n#         outputs = self.distilbert(input_ids=inputs, attention_mask=attention_mask)\n#         embeddings, output2 = torch.split(self.dropout(outputs.last_hidden_state), input_ids.size(0), dim=0)\n#         verb_idx = 0\n#         nodes, node_counts = [], []\n#         #breakpoint()\n#         for i, sample_input_ids in enumerate(input_ids):\n\n#             sample_embeddings = embeddings[i]\n#               # [sequence_length, hidden_size]\n#             sep_positions = (sample_input_ids == self.special_token).nonzero(as_tuple=True)[0] # Posizioni dei [SEP]\n\n\n#             # Suddividi gli embedding basandoti sulle posizioni dei [SEP]\n#             start, num_nodes, verb_embedding = 0, 0, []\n#             k, next_verb = 0, verb_ids[verb_idx][0] if 0 < len(verb_ids[verb_idx]) else -1\n#             for j, sep_pos in enumerate(sep_positions):\n#                 if sep_pos != start:\n#                     frase_embedding = sample_embeddings[start:sep_pos].mean(dim=0)\n#                     if j == next_verb:\n#                         verb_embedding.append(frase_embedding)\n#                         if k + 1 >= len(verb_ids[verb_idx]):\n#                             nodes.append(torch.stack(verb_embedding).mean(dim=0))#nodes.append(torch.tensor(verb_embedding).mean(dim=0))\n#                             num_nodes += 1\n#                             verb_embedding, k = [], 0\n#                             verb_idx += 1\n#                         else:\n#                             k += 1\n#                         next_verb = verb_ids[verb_idx][k]\n#                     else:\n#                         nodes.append(frase_embedding)\n#                         num_nodes += 1\n#                     start = sep_pos + 1\n#             node_counts.append(num_nodes)\n\n#         #breakpoint()\n#         x = self.gnn(torch.stack(nodes), edges)\n#         x = self.batchnorm_gnn(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         # x = self.gnn1(x, edges)\n#         # x = self.relu(x)\n#         # x = self.dropout(x)\n#         x = self.gnn2(x, edges)\n\n#         pooled_embeddings = []\n\n#         # Indice iniziale per fare slicing sugli embedding\n#         start_idx = 0\n#         #breakpoint()\n#         # Scorri sulla lista di node_counts\n#         for num_nodes in node_counts:\n#             # Prendi gli embedding dei nodi per la frase corrente\n#             current_nodes = x[start_idx:start_idx + num_nodes]\n\n#             # Aggiungi l'embedding medio alla lista\n#             pooled_embeddings.append(current_nodes.mean(dim=0))\n\n#             # Aggiorna l'indice iniziale per la frase successiva\n#             start_idx += num_nodes\n#         #breakpoint()\n\n#         # Concatena gli embedding medi per ottenere un tensore 2D di dimensione (batch_size, hidden_size)\n#         pooled_embeddings = torch.stack(pooled_embeddings)\n\n#         pooled_embeddings = torch.cat((pooled_embeddings, output2[:, 0, :]), dim=1)\n\n#         to_classify = self.relu(self.batchnorm_linear(self.linear(pooled_embeddings)))\n#         # to_classify = self.dropout(to_classify)\n#         # to_classify = self.relu(self.linear2(to_classify))\n#         to_classify = self.dropout(to_classify)\n\n#         out = self.classifier(to_classify)\n\n#         return out\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YC7tObDAhLlF","outputId":"152285e2-9ee1-49c0-d170-8dbf023afa4f","execution":{"iopub.status.busy":"2024-10-21T16:46:34.078112Z","iopub.execute_input":"2024-10-21T16:46:34.078973Z","iopub.status.idle":"2024-10-21T16:46:34.108731Z","shell.execute_reply.started":"2024-10-21T16:46:34.078919Z","shell.execute_reply":"2024-10-21T16:46:34.107519Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\n\n# class Trainer():\n\n#     def __init__(self, model,train_dataloader, validation_dataloader, optimizer, loss_function, device, scheduler=None):\n#         self.model = model.to(device)\n#         self.train_dataloader = train_dataloader\n#         self.validation_dataloader = validation_dataloader\n#         self.optimizer = optimizer\n#         self.loss_function = loss_function\n#         self.device = device\n#         self.scheduler = scheduler\n\n\n#     @staticmethod\n#     def evaluation_parameters(y_true, y_pred):\n#         #breakpoint()\n#         y_pred = np.argmax(y_pred, axis=1)\n#         cm = confusion_matrix(y_true, y_pred)\n#         precision = precision_score(y_true, y_pred, average='weighted')\n#         recall = recall_score(y_true, y_pred, average='weighted')\n#         f1 = f1_score(y_true, y_pred, average='weighted')\n#         accuracy = accuracy_score(y_true, y_pred)\n#         return cm, precision, recall, f1, accuracy\n\n#     @staticmethod\n#     def format_time_delay(seconds):\n#         hours = seconds // 3600\n#         minutes = (seconds % 3600) // 60\n#         seconds = seconds % 60\n#         return hours, minutes, seconds\n\n\n#     def train(self, epochs: int, use_wandb: bool = False, config: dict = {}, name: str=\"\", target_f1: float=0.0):\n#         start_time = time.time()\n#         best_model = None\n#         old_name = ''\n#         if use_wandb:\n#             wandb.init(\n#                 # Set the project where this run will be logged\n#                 project=\"nlphw2\",\n#                 name=name,\n#                 # Track hyperparameters and run metadata\n#                 config=config\n#             )\n#         validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n#         total_loss = validation_loss\n#         if use_wandb:\n#                 wandb.log({\"validation_loss\": validation_loss,\n#                       \"precision\": precision,\n#                       \"recall\": recall,\n#                       \"f1\": f1,\n#                       \"accuracy\": accuracy,\n#                       \"train_loss\": total_loss / len(self.train_dataloader)})\n#         for epoch in range(epochs):\n\n#             time_delay = time.time() - start_time\n#             hours, minutes, seconds = self.format_time_delay(time_delay)\n#             print(f\"\\nTempo trascorso: {hours} ore, {minutes} minuti, {seconds} secondi\")\n#             self.model.freeze(epoch)\n#             self.model.train()  # Set the model to training mode\n#             total_loss = 0\n#             #breakpoint()\n#             for i, batch in enumerate(self.train_dataloader):\n#                 print_progress_bar(i / len(self.train_dataloader), text=f\" | training epoch {epoch}\")\n#                 # Get the inputs and targets from the batch\n#                 inputs, mask, targets, edges, verbs, classic_inputs = batch\n\n#                 # Zero the gradients\n#                 self.optimizer.zero_grad()\n#                 # Forward pass\n#                 outputs = self.model(inputs, mask,  edges, verbs, classic_inputs)\n#                 #print(\"outputs = \", outputs,\"\\ntargets = \", targets)\n#                 #breakpoint()\n#                 # Compute loss\n#                 loss = self.loss_function(outputs, targets)\n#                 #print(loss)\n#                 # Backward pass and optimize\n#                 loss.backward()\n#                 self.optimizer.step()\n#                 # Accumulate the total loss\n#                 total_loss += loss.item()\n\n#             # Print the average loss for this epoch\n#             validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n#             if f1 > target_f1:\n#                 best_model = self.model.state_dict()\n#                 target_f1 = f1\n#                 if old_name != '':\n#                     os.remove(old_name)\n#                 old_name = name + f'-{target_f1}.pth'\n#                 torch.save(best_model, name + f'-{target_f1}.pth')\n#             if use_wandb:\n#                 wandb.log({\"validation_loss\": validation_loss,\n#                       \"precision\": precision,\n#                       \"recall\": recall,\n#                       \"f1\": f1,\n#                       \"accuracy\": accuracy,\n#                       \"train_loss\": total_loss / len(self.train_dataloader)})\n#             if self.scheduler is not None:\n#                 self.scheduler.step()\n#         print('\\nbest f1: ', target_f1)\n#         if use_wandb:\n#             wandb.finish()\n\n\n\n#     def validate(self, use_wandb: bool = False, test_dataloader=None, load_from=''):\n#         if os.path.isfile(load_from):\n#             self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n#         dataloader = self.validation_dataloader if test_dataloader is None else test_dataloader\n#         if dataloader is None:\n#             print(\"empty dataloader!\")\n#             exit(1)\n#         self.model.eval()  # Set the model to evaluation mode\n#         total_loss = 0\n#         all_predictions = torch.tensor([])\n#         all_targets = torch.tensor([])\n#         with torch.no_grad():  # Do not calculate gradients\n#             for i, batch in enumerate(dataloader):\n#                 print_progress_bar(i / len(dataloader), text=\" | validation\")\n#                 # Get the inputs and targets from the batch\n#                 inputs, mask, targets,  edges, verbs, classic_inputs  = batch\n\n#                 # Forward pass\n#                 #breakpoint()\n#                 outputs = self.model(inputs, mask,  edges, verbs, classic_inputs)\n#                 # Compute loss\n#                 #breakpoint()\n#                 loss = self.loss_function(outputs, targets)\n#                 # Accumulate the total loss\n#                 total_loss += loss.item()\n#                 # Store predictions and targets\n#                 all_predictions = torch.cat((all_predictions, outputs.squeeze().round().cpu()))\n#                 all_targets = torch.cat((all_targets, targets.cpu()))\n#         validation_loss = total_loss / len(self.validation_dataloader)\n#         #breakpoint()\n#         cm, precision, recall, f1, accuracy = self.evaluation_parameters(all_targets, all_predictions)\n#         return validation_loss, precision, recall, f1, accuracy, cm\n\n\n","metadata":{"id":"6d02JsMlhdCG","execution":{"iopub.status.busy":"2024-10-21T16:46:34.110195Z","iopub.execute_input":"2024-10-21T16:46:34.110726Z","iopub.status.idle":"2024-10-21T16:46:34.125237Z","shell.execute_reply.started":"2024-10-21T16:46:34.110671Z","shell.execute_reply":"2024-10-21T16:46:34.124080Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\n\nclass NLIDataset(Dataset):\n\n\n\n    def __init__(self, data, file_name = '', adversarial = False, do_remove_stopwords = False,\n                 do_remove_punctuation = False, do_use_similarities = False, base_set = True, do_lemmatization = False):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.encode_labels = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        self.model = RobertaModel.from_pretrained('roberta-base').to(self.device)\n        self.distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n        self.file_name = file_name\n        self.do_use_similarities = do_use_similarities\n        self.do_lemmatization = do_lemmatization\n        self.do_remove_punctuation = do_remove_punctuation\n        self.base_set = base_set\n        self.do_remove_stopwords = do_remove_stopwords\n        self.adversarial = adversarial\n        self.preprocess_function(data)\n        self.tokenizer = None\n        self.model = None\n\n\n\n    def remove_stopwords(self, sample):\n        stop_words = set(stopwords.words('english')) if self.do_remove_stopwords else set()\n        sentence = ''\n        for data in sample:\n            word = data\n            is_punct = False\n            if self.base_set:\n                is_punct = data['pos'] in ['PUNCT', 'CCONJ', 'DET', 'AUX']\n                word = data['lemma']\n            if word not in stop_words and not is_punct:\n                next_word = word\n                if self.do_lemmatization and self.base_set:\n                    next_word = data['lemma']\n                elif self.base_set:\n                    next_word = data['text']\n                sentence += next_word + ' '\n        return sentence.strip()\n\n\n    def preprocess_function(self, examples):\n        answers = []\n        premise_hypothesis = []\n        ordered_similarities = []\n        file_exists = self.file_name != '' #and os.path.isfile(\"data/\" + self.file_name)\n        if file_exists:\n            with open(self.file_name, \"r\") as f:\n                data_loaded = json.load(f)\n            premise_hypothesis = data_loaded[\"premise_hypotesis\"]\n            answers = data_loaded[\"labels\"]\n            ordered_similarities = data_loaded[\"similarities\"] if \"similarities\" in data_loaded else {}\n\n        # Utilizza un ciclo for per popolare le tre liste\n        else:\n        #breakpoint()\n            for i,example in enumerate(examples):\n\n                print_progress_bar(i / len(examples), text=\" | preprocessing\")\n                if (self.do_remove_stopwords or self.do_remove_punctuation) and self.base_set:\n                    premise_hypothesis.append(self.remove_stopwords(example['wsd'][\"premise\"]) + '[SEP]' + self.remove_stopwords(example['wsd'][\"hypothesis\"]))\n                    \n                elif self.do_remove_stopwords or self.do_remove_punctuation:\n                    continue\n                else:\n                    premise_hypothesis.append(example[\"premise\"].strip() + '[SEP]' + example[\"hypothesis\"].strip())\n                    \n                answers.append(self.encode_labels[example[\"label\"]] )\n\n                if self.do_use_similarities:\n                    s1 = self.embed_sentence(example[\"premise\"].strip())\n                    s2 = self.embed_sentence(example[\"hypothesis\"].strip())\n                    if self.adversarial:\n                        ordered_similarities.append(cosine_similarity(s1, s2).item())\n                    else:\n                        ordered_similarities.append(cosine_similarity(s1, s2).item())\n        #breakpoint()\n        #print(premise_hypothesis[0])\n        inputs = {}\n        inputs['inputs'] = premise_hypothesis\n        data_to_save = {\"premise_hypotesis\": premise_hypothesis, \"labels\": answers}\n        if self.do_use_similarities:\n            data_to_save[\"similarities\"] = ordered_similarities\n            inputs[\"similarity\"] = torch.tensor(ordered_similarities)\n        if not file_exists and self.file_name != '':\n            if not os.path.exists(\"data\"):\n                os.makedirs(\"data\")\n            with open(\"data/\" + self.file_name, \"w\") as f:\n                json.dump(data_to_save, f, indent=4)\n        inputs[\"label\"] = torch.tensor(answers)\n        self.data = inputs\n\n\n    def embed_sentence(self, sentence):\n        # Tokenizza la frase\n        inputs = self.tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n        # Ottieni gli embedding dal modello\n        with torch.no_grad():\n            outputs = self.model(**inputs.to(self.device))\n        # Usa l'output del modello come embedding (puoi usare altri livelli o combinazioni se preferisci)\n        # Prendi il vettore medio (puoi anche scegliere il vettore della [CLS] token, ecc.)\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n        return embeddings\n\n    def __len__(self):\n        return len(self.data[\"label\"])\n\n    def __getitem__(self, idx):\n        if \"similarity\" in self.data:\n            return self.data['inputs'][idx], self.data[\"label\"][idx], self.data[\"similarity\"][idx]\n        else:\n            return self.data['inputs'][idx], self.data[\"label\"][idx], torch.zeros(1)\n\n    def collate(self, batch):\n        #breakpoint()\n        x = []\n        attention_mask = []\n        y = []\n        z = []\n        for x_batch, y_batch, z_batch in batch:\n            x.append(x_batch)\n            y.append(y_batch)\n            z.append(z_batch)\n\n        x = self.distilbert_tokenizer(\n            x,\n            max_length = 512,\n            return_offsets_mapping=True,\n            padding='max_length',\n            truncation = True,\n            return_tensors=\"pt\"\n        )\n        attention_mask = x['attention_mask']\n        x = x['input_ids']\n        y = torch.stack(y)\n        z = torch.stack(z)\n        #breakpoint()\n        # x = pad_sequence(x, batch_first=True)\n        # attention_mask = pad_sequence(attention_mask, batch_first=True)\n        return x.to(self.device), attention_mask.to(self.device), y.to(self.device), z.to(self.device)\n\n\n\n    def get_dataloader(self, batch_size):\n        return DataLoader(self, batch_size=batch_size, shuffle=True, collate_fn = self.collate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"H5qtdZ12YpKJ","execution":{"iopub.status.busy":"2024-10-21T16:46:34.127319Z","iopub.execute_input":"2024-10-21T16:46:34.127772Z","iopub.status.idle":"2024-10-21T16:46:34.162416Z","shell.execute_reply.started":"2024-10-21T16:46:34.127733Z","shell.execute_reply":"2024-10-21T16:46:34.159756Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# prompt: create a torch model using roberta and a linear layer\n\nclass RobertaClassifier(nn.Module):\n  def __init__(self, use_similarity, num_labels=3, dropout = 0):\n    super(RobertaClassifier, self).__init__()\n    self.distilbert =  AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n    self.linear = nn.Sequential(\n        nn.Dropout(dropout),\n        nn.Linear(self.distilbert.config.hidden_size+1, 512),\n        nn.LayerNorm(512),\n        nn.LeakyReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(512, 128),\n        nn.LayerNorm(128),\n        nn.LeakyReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(128, 16),\n        nn.LayerNorm(16),\n        nn.LeakyReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(16, num_labels)\n    )\n\n    self.use_similarity = use_similarity\n    self.initialize_weights()\n\n  def initialize_weights(self):\n    # Funzione di inizializzazione dei pesi\n    for layer in self.linear:\n      if isinstance(layer, nn.Linear):\n          # Inizializzazione dei pesi con una distribuzione uniforme\n          nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n          # Inizializzazione dei bias a zero\n          if layer.bias is not None:\n              nn.init.zeros_(layer.bias)\n\n  def freeze(self, epoch):\n    if epoch == 2:\n        freeze_until_layer = 99999\n    else:\n        freeze_until_layer = None  # Non congela nulla\n\n    # Congela i layer di DistilBERT\n    if freeze_until_layer is not None:\n        for idx, param in enumerate(self.distilbert.parameters()):\n            if idx < freeze_until_layer:\n                param.requires_grad = False\n            else:\n                param.requires_grad = True\n\n\n\n  def forward(self, input_ids, attention_mask, similarities):\n\n   # breakpoint()\n    if not self.use_similarity:\n        similarities = torch.zeros(input_ids.shape[0]).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n    outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n    #breakpoint()\n\n    pooled_output = torch.cat((outputs.last_hidden_state[:, 0, :], similarities.unsqueeze(1)), dim=1)\n    # outputs.last_hidden_state.mean(dim=-1)\n\n    logits = self.linear(pooled_output)\n\n    return logits\n","metadata":{"id":"4wAtQ4CWvtV-","execution":{"iopub.status.busy":"2024-10-21T16:46:34.164908Z","iopub.execute_input":"2024-10-21T16:46:34.165700Z","iopub.status.idle":"2024-10-21T16:46:34.181773Z","shell.execute_reply.started":"2024-10-21T16:46:34.165651Z","shell.execute_reply":"2024-10-21T16:46:34.180655Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n\nclass Trainer():\n\n    def __init__(self, model,train_dataloader, validation_dataloader, optimizer, loss_function, device, scheduler=None):\n        self.model = model.to(device)\n        self.train_dataloader = train_dataloader\n        self.validation_dataloader = validation_dataloader\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.device = device\n        self.scheduler = scheduler\n\n\n    @staticmethod\n    def evaluation_parameters(y_true, y_pred):\n        #breakpoint()\n        y_pred = np.argmax(y_pred, axis=1)\n        cm = confusion_matrix(y_true, y_pred)\n        precision = precision_score(y_true, y_pred, average='weighted')\n        recall = recall_score(y_true, y_pred, average='weighted')\n        f1 = f1_score(y_true, y_pred, average='weighted')\n        accuracy = accuracy_score(y_true, y_pred)\n        return cm, precision, recall, f1, accuracy\n\n    @staticmethod\n    def format_time_delay(seconds):\n        hours = seconds // 3600\n        minutes = (seconds % 3600) // 60\n        seconds = seconds % 60\n        return hours, minutes, seconds\n\n\n    def train(self, epochs: int, use_wandb: bool = False, config: dict = {}, name: str=\"\", target_f1: float=0.0):\n        start_time = time.time()\n        best_model = None\n        old_name = ''\n        if use_wandb:\n            wandb.init(\n                # Set the project where this run will be logged\n                project=\"nlphw2\",\n                name=name,\n                # Track hyperparameters and run metadata\n                config=config\n            )\n        validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n        total_loss = validation_loss\n        if use_wandb:\n                wandb.log({\"validation_loss\": validation_loss,\n                      \"precision\": precision,\n                      \"recall\": recall,\n                      \"f1\": f1,\n                      \"accuracy\": accuracy,\n                      \"train_loss\": total_loss / len(self.train_dataloader)})\n        for epoch in range(epochs):\n\n            time_delay = time.time() - start_time\n            hours, minutes, seconds = self.format_time_delay(time_delay)\n            print(f\"\\nTempo trascorso: {hours} ore, {minutes} minuti, {seconds} secondi\")\n            self.model.freeze(epoch)\n            self.model.train()  # Set the model to training mode\n            total_loss = 0\n            #breakpoint()\n            for i, batch in enumerate(self.train_dataloader):\n                print_progress_bar(i / len(self.train_dataloader), text=f\" | training epoch {epoch}\")\n                # Get the inputs and targets from the batch\n                inputs, mask, targets, similarities = batch\n\n                # Zero the gradients\n                self.optimizer.zero_grad()\n                # Forward pass\n                outputs = self.model(inputs, mask, similarities)\n                #print(\"outputs = \", outputs,\"\\ntargets = \", targets)\n                #breakpoint()\n                # Compute loss\n                loss = self.loss_function(outputs, targets)\n                #print(loss)\n                # Backward pass and optimize\n                loss.backward()\n                self.optimizer.step()\n                # Accumulate the total loss\n                total_loss += loss.item()\n\n            # Print the average loss for this epoch\n            validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n            if f1 > target_f1:\n                best_model = self.model.state_dict()\n                target_f1 = f1\n                if old_name != '':\n                    os.remove(old_name)\n                old_name = name + f'-{target_f1}.pth'\n                torch.save(best_model, name + f'-{target_f1}.pth')\n            if use_wandb:\n                wandb.log({\"validation_loss\": validation_loss,\n                      \"precision\": precision,\n                      \"recall\": recall,\n                      \"f1\": f1,\n                      \"accuracy\": accuracy,\n                      \"train_loss\": total_loss / len(self.train_dataloader)})\n            if self.scheduler is not None:\n                self.scheduler.step()\n        print('\\nbest f1: ', target_f1)\n        if use_wandb:\n            wandb.finish()\n\n\n\n    def validate(self, use_wandb: bool = False, test_dataloader=None, load_from=''):\n        if os.path.isfile(load_from):\n            self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n        dataloader = self.validation_dataloader if test_dataloader is None else test_dataloader\n        if dataloader is None:\n            print(\"empty dataloader!\")\n            exit(1)\n        self.model.eval()  # Set the model to evaluation mode\n        total_loss = 0\n        all_predictions = torch.tensor([])\n        all_targets = torch.tensor([])\n        with torch.no_grad():  # Do not calculate gradients\n            for i, batch in enumerate(dataloader):\n                print_progress_bar(i / len(dataloader), text=\" | validation\")\n                # Get the inputs and targets from the batch\n                inputs, mask, targets, similarities  = batch\n\n                # Forward pass\n                outputs = self.model(inputs, mask, similarities)\n                # Compute loss\n                #breakpoint()\n                loss = self.loss_function(outputs, targets)\n                # Accumulate the total loss\n                total_loss += loss.item()\n                # Store predictions and targets\n                all_predictions = torch.cat((all_predictions, outputs.squeeze().round().cpu()))\n                all_targets = torch.cat((all_targets, targets.cpu()))\n        validation_loss = total_loss / len(self.validation_dataloader)\n        #breakpoint()\n        cm, precision, recall, f1, accuracy = self.evaluation_parameters(all_targets, all_predictions)\n        return validation_loss, precision, recall, f1, accuracy, cm\n\n\n","metadata":{"id":"dcK9u6vN8qQf","execution":{"iopub.status.busy":"2024-10-21T16:46:34.183408Z","iopub.execute_input":"2024-10-21T16:46:34.183928Z","iopub.status.idle":"2024-10-21T16:46:34.212013Z","shell.execute_reply.started":"2024-10-21T16:46:34.183879Z","shell.execute_reply":"2024-10-21T16:46:34.210773Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class BaselineStratifiedModel(nn.Module):\n\n    def __init__(self, len0, len1):\n        super(BaselineStratifiedModel, self).__init__()\n        self.p = len0/(len0+len1)\n\n    def forward(self, x):\n        return torch.tensor([0 if np.random.rand() < self.p else 1 for _ in range(x[0].shape[0])], dtype=torch.float)\n\n","metadata":{"id":"cnqcVF1m7Gx6","execution":{"iopub.status.busy":"2024-10-21T16:46:34.214026Z","iopub.execute_input":"2024-10-21T16:46:34.214786Z","iopub.status.idle":"2024-10-21T16:46:34.226117Z","shell.execute_reply.started":"2024-10-21T16:46:34.214740Z","shell.execute_reply":"2024-10-21T16:46:34.224934Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"new_seed = 108\ndef set_seed(seed):\n    np.random.seed(seed)\n    rnd.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True # Se stai usando GPU\n    return seed, seed+1","metadata":{"id":"uF0IzCm6Rk8O","execution":{"iopub.status.busy":"2024-10-21T16:46:34.227634Z","iopub.execute_input":"2024-10-21T16:46:34.228080Z","iopub.status.idle":"2024-10-21T16:46:34.239698Z","shell.execute_reply.started":"2024-10-21T16:46:34.228010Z","shell.execute_reply":"2024-10-21T16:46:34.238659Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\n\n# Imposta il seed per avere riproducibilità\n\nseed, new_seed = set_seed(new_seed)","metadata":{"id":"Tex3y-pe6ttW","execution":{"iopub.status.busy":"2024-10-21T16:46:34.241256Z","iopub.execute_input":"2024-10-21T16:46:34.241811Z","iopub.status.idle":"2024-10-21T16:46:34.255052Z","shell.execute_reply.started":"2024-10-21T16:46:34.241755Z","shell.execute_reply":"2024-10-21T16:46:34.253952Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nadversarial = load_dataset(\"iperbole/adversarial_fever_nli\")[\"test\"]\n\nds = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n\ntraining_set = ds[\"train\"]\n\nvalidation_set = ds[\"validation\"]\n\ntest_set = ds[\"test\"]\n\nwandb.login(key='aaf831dabc88d936d4e6b439b798bb4cb42814ea')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MTLav9Ttb7q","outputId":"a5d70f3f-0f3a-4fc7-cf82-75b1ddf3cb46","execution":{"iopub.status.busy":"2024-10-21T16:46:34.256655Z","iopub.execute_input":"2024-10-21T16:46:34.257002Z","iopub.status.idle":"2024-10-21T16:46:44.969299Z","shell.execute_reply.started":"2024-10-21T16:46:34.256967Z","shell.execute_reply":"2024-10-21T16:46:44.968144Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07747d382d9419582a71576ff2b9280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/73.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0efdf7e90324432a4268d2fd332f6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/337 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41d5504a5f5437193bc36205b502d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37b98d85f674039beae2dc6c1cf4a79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/72.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f955ff737f4f6aa18565d8abc02b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/3.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ed62e79a1743da99c1227b643028cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/3.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a11989d4d842298e1b4eefec5217c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2138b68e6e674d11b703b9198e9375a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2288 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e313f637f4414ce891cc4e3ab6204eba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b8f1b537c841a8b07d4cd88d4c941f"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# !git clone https://github.com/monteleone-1883922/hw2_nlp.git\n# os.chdir(\"hw2_nlp\")\n# !git checkout huggingFaceBase\n# !mv data ./../data\n# os.chdir(\"..\")\n# !rm -rf hw2_nlp\nadversarial[0]","metadata":{"id":"Xx8K6p8vvKBN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18f6cea8-cebd-4815-9088-adf4ebfded43","execution":{"iopub.status.busy":"2024-10-21T16:46:44.970708Z","iopub.execute_input":"2024-10-21T16:46:44.971919Z","iopub.status.idle":"2024-10-21T16:46:44.981326Z","shell.execute_reply.started":"2024-10-21T16:46:44.971869Z","shell.execute_reply":"2024-10-21T16:46:44.980325Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'part': 'manual_adversarial',\n 'cid': 58846,\n 'premise': 'Johnny Galecki . He is known for playing David Healy in the ABC sitcom Roseanne from 1992 -- 1997 and Dr. Leonard Hofstadter in the CBS sitcom The Big Bang Theory since 2007 .',\n 'hypothesis': 'The number of sitcoms from France in which Johnny Galecki has played a character is greater or equal to 2',\n 'label': 'NEUTRAL'}"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"id":"cONolUaUahmF"}},{"cell_type":"code","source":"train_dataset = NLIDataset(training_set, \"/kaggle/input/basemodeldataset/training.json\")#, load=True)#, do_remove_punctuation=True)\n\nvalidation_dataset = NLIDataset(validation_set, \"/kaggle/input/basemodeldataset/validation.json\")#, load=True)#, do_remove_punctuation=True)\n\ntest_dataset = NLIDataset(test_set, \"/kaggle/input/basemodeldataset/test.json\")#, load=True)#, do_remove_punctuation=True)\n\n#adversarial_dataset = NLIDataset(adversarial, \"erwwwrre4.json\")#, adversarial=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9j_yPVcNvUzd","outputId":"542edd15-7219-4a89-a771-a56d9af92e2a","execution":{"iopub.status.busy":"2024-10-21T16:46:44.982556Z","iopub.execute_input":"2024-10-21T16:46:44.982910Z","iopub.status.idle":"2024-10-21T16:47:00.757189Z","shell.execute_reply.started":"2024-10-21T16:46:44.982877Z","shell.execute_reply":"2024-10-21T16:47:00.756237Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a27a2cdaac246729505d338ff1a0929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc9c75655bb4a70b05cfc0c4c810d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9ffdb683c440f89b52d66df4b802af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d48e49e9810b45e19e6eaa4e34a969c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060712fea8b44d5681b88e37f9e11466"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832ce44f4aa44795b6d3df2df1f54ba6"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7acb1303f2d446db9fc308e1dfcb4ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0d20dd698e34f278b9cbd0086d12498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5701b5334ace47f297b4d5231d6c5a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c0010108c647d89384d87603e736af"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# prompt: genera i dataloader\n\ntrain_dataloader = train_dataset.get_dataloader(batch_size=40)\nvalidation_dataloader = validation_dataset.get_dataloader(batch_size=40)\n#test_dataloader = test_dataset.get_dataloader(batch_size=40)\n#adversarial_dataloader = adversarial_dataset.get_dataloader(batch_size=32)\n","metadata":{"id":"GkXZIxkPvVrW","execution":{"iopub.status.busy":"2024-10-21T16:47:00.759096Z","iopub.execute_input":"2024-10-21T16:47:00.760052Z","iopub.status.idle":"2024-10-21T16:47:00.765580Z","shell.execute_reply.started":"2024-10-21T16:47:00.760005Z","shell.execute_reply":"2024-10-21T16:47:00.764397Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\nmodel = RobertaClassifier(False, dropout=0.3)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-5)\n\ntrainer = Trainer(model, train_dataloader, validation_dataloader, optimizer, nn.CrossEntropyLoss(),'cuda' if torch.cuda.is_available() else 'cpu')#, scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[2],0.2))\n\n","metadata":{"id":"zZdBugO7u5pW","execution":{"iopub.status.busy":"2024-10-21T16:47:00.766788Z","iopub.execute_input":"2024-10-21T16:47:00.767118Z","iopub.status.idle":"2024-10-21T16:47:02.203102Z","shell.execute_reply.started":"2024-10-21T16:47:00.767084Z","shell.execute_reply":"2024-10-21T16:47:02.202274Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c79a7a0fdb4212bd34ba1a5f908ca6"}},"metadata":{}}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/basemodeldataset/deep_base_batch_adamW_5e-5_108-0.7295309063203738.pth\"))\n# model.freeze(1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAdbhXtvCMIg","outputId":"99153a44-0043-4d52-dd48-6e78ec6300a7","execution":{"iopub.status.busy":"2024-10-21T16:47:02.204303Z","iopub.execute_input":"2024-10-21T16:47:02.204641Z","iopub.status.idle":"2024-10-21T16:47:05.003226Z","shell.execute_reply.started":"2024-10-21T16:47:02.204607Z","shell.execute_reply":"2024-10-21T16:47:05.001978Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2240868002.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/basemodeldataset/deep_base_batch_adamW_5e-5_108-0.7295309063203738.pth\"))\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"#!rm -rf data","metadata":{"id":"Yot5Bec8KbUA","execution":{"iopub.status.busy":"2024-10-21T16:47:05.004565Z","iopub.execute_input":"2024-10-21T16:47:05.004957Z","iopub.status.idle":"2024-10-21T16:47:05.009872Z","shell.execute_reply.started":"2024-10-21T16:47:05.004922Z","shell.execute_reply":"2024-10-21T16:47:05.008612Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"lsdKlL-WyzAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.validate(test=False)","metadata":{"id":"nJk1swaCF70a","execution":{"iopub.status.busy":"2024-10-21T16:47:05.011326Z","iopub.execute_input":"2024-10-21T16:47:05.011792Z","iopub.status.idle":"2024-10-21T16:47:07.907184Z","shell.execute_reply.started":"2024-10-21T16:47:05.011735Z","shell.execute_reply":"2024-10-21T16:47:07.905788Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\ntrainer.train(5, use_wandb=True, name=\"base_deep_batch_adamW_5e-5_\"+ str(seed))\n\n\n","metadata":{"id":"ASCjcQMQvw9O","colab":{"base_uri":"https://localhost:8080/","height":160},"outputId":"a9d32a6f-0b11-457b-c714-57d197523258","_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-10-21T16:47:07.908830Z","iopub.execute_input":"2024-10-21T16:47:07.909286Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmonteleone-1883922\u001b[0m (\u001b[33mmonteleone\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113737255555457, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e8cb4f3d8d4878b25e7561011390de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241021_164708-iio021bg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/monteleone/nlphw2/runs/iio021bg' target=\"_blank\">base_deep_batch_adamW_5e-5_108</a></strong> to <a href='https://wandb.ai/monteleone/nlphw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/monteleone/nlphw2' target=\"_blank\">https://wandb.ai/monteleone/nlphw2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/monteleone/nlphw2/runs/iio021bg' target=\"_blank\">https://wandb.ai/monteleone/nlphw2/runs/iio021bg</a>"},"metadata":{}},{"name":"stdout","text":"[============================> ] 98.28% complete  | validation\nTempo trascorso: 0.0 ore, 0.0 minuti, 26.346377849578857 secondi\n[============================> ] 98.28% complete  | validationpoch 0\nTempo trascorso: 0.0 ore, 24.0 minuti, 3.719756841659546 secondi\n[============================> ] 98.28% complete  | validationpoch 1\nTempo trascorso: 0.0 ore, 47.0 minuti, 41.644123792648315 secondi\n[=>                            ] 7.04% complete  | training epoch 22","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"T-uPr98CKaAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_loss_1, precision_1, recall_1, f1_1, accuracy_1, cm_1 = trainer.validate(test_dataloader=test_dataloader, load_from='simil_1e-5_108-0.7299378113069238.pth')\n\nvalidation_loss_2, precision_2, recall_2, f1_2, accuracy_2, cm_2 = trainer.validate(test_dataloader=adversarial_dataloader, load_from='simil_1e-5_108-0.7299378113069238.pth')\n","metadata":{"tags":[],"id":"86fceafe-aa0b-4cf5-a81d-68a7bdbd1089","outputId":"a7bddd5f-aad1-462d-9b8a-29b10f64c8cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f1_1)\nprint(f1_2)\nprint(type(cm_1))\n\n# Visualizza la matrice di confusione\n","metadata":{"tags":[],"id":"4dc0747c-5d1d-4dc9-bc5e-e47a8753ad9f","outputId":"2ae2dad1-24ca-45e2-93b7-c3bcf72cd15e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=['CONTRADICTION', 'NEUTRAL','ENTAILMENT' ])\ndisp.plot()\n\n# Se stai lavorando in un ambiente interattivo come Jupyter Notebook, la matrice verrà visualizzata automaticamente\n# Se sei in uno script Python, puoi usare plt.show() per visualizzare la matrice\n\nplt.show()","metadata":{"tags":[],"id":"32f2992e-43bf-465e-a11a-54d1189b961c","outputId":"0c20a69e-5a81-41ff-84d9-574ff470a9ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm_2, display_labels=['CONTRADICTION', 'NEUTRAL','ENTAILMENT' ])\ndisp.plot()\n\nplt.show()","metadata":{"tags":[],"id":"b63b6999-7500-4d8d-bc09-4287111f4c0a","outputId":"9f085008-d094-492d-da71-2c8c29e321af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del training_set\n\n# dataset = NLIDataset(validation_set)\n\n\n# dataloader = dataset.get_dataloader(batch_size=32, pos_num=1)\n\n# model = RobertaClassifier()\n\n# batch = next(iter(dataloader))\n\n# model(batch[0], batch[1], batch[3])","metadata":{"id":"3vUyqHDfBiYG","tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}