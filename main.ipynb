{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nHARZQ6fIXm8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHARZQ6fIXm8",
        "outputId": "74c29994-b200-45d1-c125-8dff210a5f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.16.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (6.29.5)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.3)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Cloning into 'hw2_nlp'...\n",
            "remote: Enumerating objects: 307, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 307 (delta 95), reused 55 (delta 26), pack-reused 159 (from 1)\u001b[K\n",
            "Receiving objects: 100% (307/307), 1.17 MiB | 5.52 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "/content/hw2_nlp\n",
            "Branch 'huggingFaceBase' set up to track remote branch 'huggingFaceBase' from 'origin'.\n",
            "Switched to a new branch 'huggingFaceBase'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#INSTALL LIBRARIES------------------------------------------\n",
        "!pip install transformers scikit-learn datasets wandb word2number nltk torch_geometric\n",
        "!pip install --upgrade ipykernel\n",
        "!git clone https://github.com/monteleone-1883922/hw2_nlp.git\n",
        "%cd hw2_nlp/\n",
        "!git checkout huggingFaceBase\n",
        "%cd ..\n",
        "!cp hw2_nlp/manipulations.py .\n",
        "!cp hw2_nlp/augmentation.py .\n",
        "!rm -r hw2_nlp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aZ4RHIcXDNcA",
      "metadata": {
        "id": "aZ4RHIcXDNcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8c57ee-e0ce-436a-f150-df5810c28889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#IMPORTS-----------------------------\n",
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from torch.nn import Linear, ReLU\n",
        "import pdb\n",
        "import numpy as np, torch, random as rnd, torch.nn as nn, wandb\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import sys, os, json\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score,  ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "import os, time\n",
        "from augmentation import augment_data, augment_data_multithread\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from torch_geometric.nn.norm import BatchNorm\n",
        "import torch.nn.init as init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "initial_id",
      "metadata": {
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# adversarial = load_dataset(\"iperbole/adversarial_fever_nli\")[\"test\"]\n",
        "\n",
        "# ds = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
        "\n",
        "# training_set = ds[\"train\"]\n",
        "\n",
        "# validation_set = ds[\"validation\"]\n",
        "\n",
        "# test_set = ds[\"test\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random.seed(42)\n",
        "\n",
        "# training_set, new_data, info_augmentations = augment_data(training_set, 2000)"
      ],
      "metadata": {
        "id": "pSp_pqX7wy4_"
      },
      "id": "pSp_pqX7wy4_",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouped_data = {}\n",
        "# for sample in new_data:\n",
        "#     augment_value = sample['augment_method']\n",
        "#     grouped_data[augment_value] = grouped_data.get(augment_value, []) + [sample]\n",
        "# pprint(info_augmentations)\n",
        "\n",
        "# total_attempts = 0\n",
        "# total_successes = 0\n",
        "\n",
        "# # Itera attraverso il dizionario e somma i valori\n",
        "# for key, values in info_augmentations.items():\n",
        "#     total_attempts += values['count']\n",
        "#     total_successes += values['success']\n",
        "\n",
        "# # Stampa i risultati\n",
        "# print(f\"Totale tentativi: {total_attempts}\")\n",
        "# print(f\"Totale successi: {total_successes}\")"
      ],
      "metadata": {
        "id": "adl5-XZtM30z"
      },
      "id": "adl5-XZtM30z",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pprint(grouped_data['CHANGE_NUMBERS'][3])\n",
        "\n",
        "# NEGATE_PART_PREMISE = 1\n",
        "#     # anything -> same thing\n",
        "#     SYNONYM = 2\n",
        "#     # anything -> negation\n",
        "#     ANTINOMY_PART_PREMISE = 3\n",
        "#     # anything -> same thing\n",
        "#     HYPONYM_PREMISE = 4\n",
        "#     # anything -> neutral\n",
        "#     SWITCH_DATA = 5\n",
        "#     # anything -> neutral\n",
        "#     SWITCH_PARTIAL_DATA = 6\n",
        "#     # anything -> entailment\n",
        "#     TAKE_PART_PREMISE = 7\n",
        "#     # entailment/negation -> opposite\n",
        "#     NEGATE_HYPOTHESIS = 8\n",
        "#     # anything -> same thing\n",
        "#     HYPERNYM_HYPOTHESIS = 9\n",
        "#     # anything -> negation\n",
        "#     IMPOSSIBILITY = 10\n",
        "#     # entailment -> entailment\n",
        "#     TRUNCATE_HYPOTHESIS = 11\n",
        "#     # anything -> entailment\n",
        "#     TAUTOLOGY = 12\n",
        "#     # anything -> entailment\n",
        "#     DUPLICATE_HYPOTHESIS = 13\n",
        "#     # entailment -> negation/entailment\n",
        "#     CHANGE_NUMBERS = 14\n"
      ],
      "metadata": {
        "id": "Pg7h8xTxwoud"
      },
      "id": "Pg7h8xTxwoud",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(training_set[0]['hypothesis'])\n",
        "# print(training_set[0]['premise'])\n",
        "# print(training_set[1]['hypothesis'])\n",
        "# print(training_set[1]['premise'])\n",
        "# print(training_set[2]['hypothesis'])\n",
        "# print(training_set[2]['premise'])"
      ],
      "metadata": {
        "id": "xTXYcNXftIjd"
      },
      "id": "xTXYcNXftIjd",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "l6yGkP9JJwmn",
      "metadata": {
        "id": "l6yGkP9JJwmn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# print(stopwords.words('english'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "izN_nvlqSoUg",
      "metadata": {
        "id": "izN_nvlqSoUg"
      },
      "outputs": [],
      "source": [
        "# f1 = \"test per vedere come va\"\n",
        "# f2 = \"questa è una prova\"\n",
        "\n",
        "# tokenized = tokenizer(f1+ tokenizer.eos_token + f2, return_tensors='pt', padding='max_length', max_length=40, return_token_type_ids=True)\n",
        "\n",
        "# print(tokenized)\n",
        "# print(tokenized[\"input_ids\"].shape)\n",
        "# print(type(tokenized))\n",
        "\n",
        "# out = model(**tokenized)\n",
        "# print(out['last_hidden_state'].shape)\n",
        "\n",
        "# print(out.last_hidden_state.mean(dim=-1).squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "yUEyqUjxq8tC",
      "metadata": {
        "id": "yUEyqUjxq8tC"
      },
      "outputs": [],
      "source": [
        "# Function to print a progress bar\n",
        "def print_progress_bar(percentuale: float, lunghezza_barra: int = 30, text: str=\"\") -> None:\n",
        "    blocchi_compilati = int(lunghezza_barra * percentuale)\n",
        "    barra = \"[\" + \"=\" * (blocchi_compilati - 1) + \">\" + \" \" * (lunghezza_barra - blocchi_compilati) + \"]\"\n",
        "    sys.stdout.write(f\"\\r{barra} {percentuale * 100:.2f}% complete \" + text)\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class NLIDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, file_name='',load = False, adversarial=False, do_remove_stopwords=False,\n",
        "                 do_remove_punctuation=False, do_use_similarities=False, base_set=True, do_lemmatization=False):\n",
        "        self.sentence_info = None\n",
        "        self.labels = None\n",
        "        self.sentences = None\n",
        "        self.load = load\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.encode_labels = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.model = RobertaModel.from_pretrained('roberta-base').to(self.device)\n",
        "        self.distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "        self.file_name = file_name\n",
        "        self.do_use_similarities = do_use_similarities\n",
        "        self.do_lemmatization = do_lemmatization\n",
        "        self.do_remove_punctuation = do_remove_punctuation\n",
        "        self.base_set = base_set\n",
        "        self.do_remove_stopwords = do_remove_stopwords\n",
        "        self.adversarial = adversarial\n",
        "        self.organize_data(data)\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.count = 0\n",
        "\n",
        "    def organize_data(self, data):\n",
        "        info_samples = []\n",
        "        sentences = []\n",
        "        labels = []\n",
        "        premise_hypotesis = []\n",
        "        if not self.load or not os.path.isfile(self.file_name):\n",
        "            for num_sample, sample in enumerate(data):\n",
        "                print_progress_bar(num_sample / len(data), text=\" | loading data\")\n",
        "                #breakpoint()\n",
        "                labels.append(self.encode_labels[sample[\"label\"]])\n",
        "                premise_hypotesis.append(sample['premise'] + \" [SEP] \" + sample['hypothesis'])\n",
        "                sentence = []\n",
        "                list_sentence_premise = [token[\"rawText\"] for token in sample['srl']['premise']['tokens']]\n",
        "                list_sentence_hypothesis = [token[\"rawText\"] for token in sample['srl']['hypothesis']['tokens']]\n",
        "                info_sample = {\"ids_verbs\": [], \"edges\": []}\n",
        "                idx = 0\n",
        "                edge_idx = 0\n",
        "                verb_edge_idx = -1\n",
        "                repeated_nodes = {}\n",
        "                # premise -----------------------------\n",
        "                for annotation in sample['srl']['premise']['annotations']:\n",
        "                    if verb_edge_idx != -1:\n",
        "                        info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n",
        "                        info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n",
        "                    verb_edge_idx = edge_idx\n",
        "                    edge_idx += 1\n",
        "                    verb_word_only = True\n",
        "                    if len(sample['wsd']['premise']) > annotation['tokenIndex'] and sample['wsd']['premise'][annotation['tokenIndex']]['nltkSynset'] != 'O' and sample['wsd']['premise'][annotation['tokenIndex']]['pos'] == 'VERB':\n",
        "                        examples = wn.synset(sample['wsd']['premise'][annotation['tokenIndex']]['nltkSynset']).examples()\n",
        "                        id_verb = []\n",
        "                        for example in examples:\n",
        "                            if example != '':\n",
        "                                id_verb.append(idx)\n",
        "                                idx += 1\n",
        "                                verb_word_only = False\n",
        "                                sentence.append( example + ' [SEP] ')\n",
        "                        info_sample[\"ids_verbs\"].append(tuple(id_verb))\n",
        "                    if verb_word_only:\n",
        "                        info_sample[\"ids_verbs\"].append((idx,))\n",
        "                        idx += 1\n",
        "                        sentence.append(list_sentence_premise[annotation['tokenIndex']] + ' [SEP] ')\n",
        "                    for element in annotation['verbatlas']['roles']:\n",
        "                        if element['span']:\n",
        "                            if (element['span'][0], element['span'][1]) in repeated_nodes:\n",
        "                                info_sample[\"edges\"].append((repeated_nodes[(element['span'][0], element['span'][1])], verb_edge_idx))\n",
        "                                info_sample[\"edges\"].append((verb_edge_idx, repeated_nodes[(element['span'][0], element['span'][1])]))\n",
        "                            else:\n",
        "                                info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n",
        "                                info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n",
        "                                edge_idx += 1\n",
        "                                idx += 1\n",
        "                                sentence.append(' '.join(list_sentence_premise[element['span'][0]:element['span'][1]]) + ' [SEP] ')\n",
        "                                repeated_nodes[(element['span'][0], element['span'][1])] = edge_idx\n",
        "                repeated_nodes.clear()\n",
        "                verb_edge_idx = -1\n",
        "                # hypothesis -----------------------------\n",
        "                for annotation in sample['srl']['hypothesis']['annotations']:\n",
        "                    if verb_edge_idx != -1:\n",
        "                        info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n",
        "                        info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n",
        "                    verb_edge_idx = edge_idx\n",
        "                    edge_idx += 1\n",
        "                    verb_word_only = True\n",
        "                    if len(sample['wsd']['hypothesis']) > annotation['tokenIndex'] and sample['wsd']['hypothesis'][annotation['tokenIndex']]['nltkSynset'] != 'O' and sample['wsd']['hypothesis'][annotation['tokenIndex']]['pos'] == 'VERB':\n",
        "                        examples = wn.synset(sample['wsd']['hypothesis'][annotation['tokenIndex']]['nltkSynset']).examples()\n",
        "                        id_verb = []\n",
        "                        for example in examples:\n",
        "                            if example != '':\n",
        "                                id_verb.append(idx)\n",
        "                                idx += 1\n",
        "                                verb_word_only = False\n",
        "                                sentence.append(example + ' [SEP] ')\n",
        "                        info_sample[\"ids_verbs\"].append(tuple(id_verb))\n",
        "                    if verb_word_only:\n",
        "                        info_sample[\"ids_verbs\"].append((idx,))\n",
        "                        idx += 1\n",
        "                        sentence.append(list_sentence_hypothesis[annotation['tokenIndex']] + ' [SEP] ')\n",
        "                    for element in annotation['verbatlas']['roles']:\n",
        "                        if element['span']:\n",
        "                            if (element['span'][0], element['span'][1]) in repeated_nodes:\n",
        "                                info_sample[\"edges\"].append(\n",
        "                                    (repeated_nodes[(element['span'][0], element['span'][1])], verb_edge_idx))\n",
        "                                info_sample[\"edges\"].append(\n",
        "                                    (verb_edge_idx, repeated_nodes[(element['span'][0], element['span'][1])]))\n",
        "                            else:\n",
        "                                info_sample[\"edges\"].append((verb_edge_idx, edge_idx))\n",
        "                                info_sample[\"edges\"].append((edge_idx, verb_edge_idx))\n",
        "                                edge_idx += 1\n",
        "                                idx += 1\n",
        "                                sentence.append(' '.join(list_sentence_premise[element['span'][0]:element['span'][1]]) + ' [SEP] ')\n",
        "                                repeated_nodes[(element['span'][0], element['span'][1])] = edge_idx\n",
        "                info_sample[\"num_nodes\"] = edge_idx\n",
        "                info_sample[\"num_sentences\"] = idx\n",
        "                sentences.append(' '.join(sentence))\n",
        "                info_samples.append(info_sample)\n",
        "            if self.file_name != '':\n",
        "                with open(self.file_name, 'w') as file:\n",
        "                    json.dump({'sentences': sentences, 'info_samples': info_samples, 'labels': labels, 'premise_hypotesis': premise_hypotesis}, file)\n",
        "        else:\n",
        "            with open(self.file_name, 'r') as file:\n",
        "                loaded_data = json.load(file)\n",
        "            sentences = loaded_data['sentences']\n",
        "            info_samples = loaded_data['info_samples']\n",
        "            labels = loaded_data['labels']\n",
        "            premise_hypotesis = loaded_data['premise_hypotesis']\n",
        "\n",
        "        self.sentences = sentences\n",
        "        self.sentence_info = info_samples\n",
        "        self.labels = labels\n",
        "        self.premise_hypotesis = premise_hypotesis\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx], self.labels[idx], self.sentence_info[idx], self.premise_hypotesis[idx]\n",
        "\n",
        "    def get_deleted_per_epoch(self, num_epochs):\n",
        "        out = self.count / num_epochs\n",
        "        self.count = 0\n",
        "        return out\n",
        "\n",
        "    def collate(self, batch):\n",
        "        #breakpoint()\n",
        "        inputs = []\n",
        "        gold_outputs = []\n",
        "        edges = []\n",
        "        verb_ids = []\n",
        "        offset_edges = 0\n",
        "        offset_sentences = 0\n",
        "        num_nodes = []\n",
        "        classic_inputs = []\n",
        "        for input_batch, gold_outputs_batch, sample_info_batch, premise_hypotesis in batch:\n",
        "            if len(word_tokenize(input_batch)) < 500:\n",
        "                inputs.append(input_batch)\n",
        "                gold_outputs.append(gold_outputs_batch)\n",
        "                classic_inputs.append(premise_hypotesis)\n",
        "                edges.extend([(x + offset_edges, y + offset_edges) for x, y in sample_info_batch[\"edges\"]])\n",
        "\n",
        "                # Convertiamo da tuple a liste\n",
        "\n",
        "                offset_edges += sample_info_batch[\"num_nodes\"]\n",
        "\n",
        "\n",
        "                verb_ids.append(sample_info_batch[\"ids_verbs\"])\n",
        "                offset_sentences += sample_info_batch[\"num_sentences\"]\n",
        "            else:\n",
        "                self.count += 1\n",
        "        #breakpoint()\n",
        "        inputs = self.distilbert_tokenizer(\n",
        "            inputs,\n",
        "            max_length=512,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        classic_inputs = self.distilbert_tokenizer(\n",
        "            classic_inputs,\n",
        "            max_length=512,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "\n",
        "        gold_outputs = torch.tensor(gold_outputs, dtype=torch.long)\n",
        "        edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        return inputs['input_ids'].to(self.device), inputs['attention_mask'].to(self.device), gold_outputs.to(self.device), \\\n",
        "            edges.to(self.device), verb_ids, classic_inputs.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size):\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=True, collate_fn=self.collate)\n",
        "\n",
        "\n",
        "# prompt: create a torch model using roberta and a linear layer\n",
        "\n",
        "class RobertaClassifier(nn.Module):\n",
        "    def __init__(self, use_similarity, num_labels=3, dropout=0):\n",
        "        super(RobertaClassifier, self).__init__()\n",
        "        self.distilbert = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gnn = GATv2Conv(self.distilbert.config.hidden_size, 64, dropout=0.3)\n",
        "        # self.gnn1 = GATConv(64, 64)\n",
        "        self.gnn2 = GATv2Conv(64, 32, dropout=0.15)  # ,heads=2)\n",
        "        self.linear = nn.Linear(self.distilbert.config.hidden_size + 32, 64)\n",
        "        #self.linear2 = nn.Linear(128, 128)\n",
        "        self.classifier = nn.Linear(64, 3)\n",
        "        self.initialize_weights()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.batchnorm_linear = nn.BatchNorm1d(64)\n",
        "        # self.batchnorm_gnn = BatchNorm(64)  # Modifica con hidden_size corretto\n",
        "        # self.batchnorm_gnn2 = BatchNorm(32)\n",
        "        self.special_token = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\").sep_token_id\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, GATv2Conv):\n",
        "                # Inizializzazione dei pesi per GATConv\n",
        "                init.kaiming_normal_(layer.lin_l.weight, a=0.01)\n",
        "\n",
        "                if layer.lin_l.bias is not None:\n",
        "                    init.zeros_(layer.lin_l.bias)\n",
        "\n",
        "\n",
        "                # Se hai anche la parte destra del layer di attenzione\n",
        "                if hasattr(layer, 'lin_r') and layer.lin_r is not None:\n",
        "                    init.kaiming_normal_(layer.lin_r.weight, a=0.01)  # destra\n",
        "                    if layer.lin_r.bias is not None:\n",
        "                        init.zeros_(layer.lin_r.bias)\n",
        "\n",
        "            elif isinstance(layer, nn.Linear):\n",
        "                # Inizializzazione Xavier per i layer lineari\n",
        "                init.kaiming_normal_(layer.weight, a=0.01)\n",
        "\n",
        "                if layer.bias is not None:\n",
        "                    init.zeros_(layer.bias)\n",
        "\n",
        "    # def extract_next_verb(verb_ids, verb_idx, k, range_sep):\n",
        "    #     new_k = -1\n",
        "    #     for i in range(verb_idx, len(verb_ids)):\n",
        "    #         out_of_range = False\n",
        "    #         for j in range(k, len(verb_ids[verb_idx])):\n",
        "    #             if verb_ids[i][j].item() >= range_sep:\n",
        "    #                 out_of_range = True\n",
        "    #                 new_k = j\n",
        "    #                 break\n",
        "    #         k = 0\n",
        "    #         if not out_of_range:\n",
        "    #             return i, new_k, verb_ids[i][0].item()\n",
        "    #     return -1,-1,-1\n",
        "\n",
        "\n",
        "    def freeze(self, epoch):\n",
        "        if epoch == 1:\n",
        "            freeze_until_layer = 99999\n",
        "        else:\n",
        "            freeze_until_layer = None  # Non congela nulla\n",
        "\n",
        "        # Congela i layer di DistilBERT\n",
        "        if freeze_until_layer is not None:\n",
        "            for idx, param in enumerate(self.distilbert.parameters()):\n",
        "                if idx < freeze_until_layer:\n",
        "                    param.requires_grad = False\n",
        "                else:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, verb_ids, classic_inputs):\n",
        "        #breakpoint()\n",
        "        inputs = torch.cat((input_ids, classic_inputs['input_ids']), dim=0)\n",
        "        attention_mask = torch.cat((attention_mask, classic_inputs['attention_mask']), dim=0)\n",
        "\n",
        "        outputs = self.distilbert(input_ids=inputs, attention_mask=attention_mask)\n",
        "        embeddings, output2 = torch.split(outputs.last_hidden_state, input_ids.size(0), dim=0)\n",
        "        verb_idx = 0\n",
        "        nodes, node_counts = [], []\n",
        "        #breakpoint()\n",
        "        for i, sample_input_ids in enumerate(input_ids):\n",
        "\n",
        "            sample_embeddings = embeddings[i]\n",
        "              # [sequence_length, hidden_size]\n",
        "            sep_positions = (sample_input_ids == self.special_token).nonzero(as_tuple=True)[0] # Posizioni dei [SEP]\n",
        "\n",
        "\n",
        "            # Suddividi gli embedding basandoti sulle posizioni dei [SEP]\n",
        "            start, num_nodes, verb_embedding = 0, 0, []\n",
        "            k, next_verb = 0, verb_ids[verb_idx][0] if 0 < len(verb_ids[verb_idx]) else -1\n",
        "            for j, sep_pos in enumerate(sep_positions):\n",
        "                if sep_pos != start:\n",
        "                    frase_embedding = sample_embeddings[start:sep_pos].mean(dim=0)\n",
        "                    if j == next_verb:\n",
        "                        verb_embedding.append(frase_embedding)\n",
        "                        if k + 1 >= len(verb_ids[verb_idx]):\n",
        "                            nodes.append(torch.stack(verb_embedding).mean(dim=0))#nodes.append(torch.tensor(verb_embedding).mean(dim=0))\n",
        "                            num_nodes += 1\n",
        "                            verb_embedding, k = [], 0\n",
        "                            verb_idx += 1\n",
        "                        else:\n",
        "                            k += 1\n",
        "                        next_verb = verb_ids[verb_idx][k]\n",
        "                    else:\n",
        "                        nodes.append(frase_embedding)\n",
        "                        num_nodes += 1\n",
        "                    start = sep_pos + 1\n",
        "            node_counts.append(num_nodes)\n",
        "\n",
        "        #breakpoint()\n",
        "        x = self.gnn(torch.stack(nodes), edges)\n",
        "        # x = self.batchnorm_gnn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # x = self.gnn1(x, edges)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.gnn2(x, edges)\n",
        "\n",
        "        pooled_embeddings = []\n",
        "\n",
        "        # Indice iniziale per fare slicing sugli embedding\n",
        "        start_idx = 0\n",
        "        #breakpoint()\n",
        "        # Scorri sulla lista di node_counts\n",
        "        for num_nodes in node_counts:\n",
        "            # Prendi gli embedding dei nodi per la frase corrente\n",
        "            current_nodes = x[start_idx:start_idx + num_nodes]\n",
        "\n",
        "            # Aggiungi l'embedding medio alla lista\n",
        "            pooled_embeddings.append(current_nodes.mean(dim=0))\n",
        "\n",
        "            # Aggiorna l'indice iniziale per la frase successiva\n",
        "            start_idx += num_nodes\n",
        "        #breakpoint()\n",
        "\n",
        "        # Concatena gli embedding medi per ottenere un tensore 2D di dimensione (batch_size, hidden_size)\n",
        "        pooled_embeddings = torch.stack(pooled_embeddings)\n",
        "\n",
        "        pooled_embeddings = torch.cat((pooled_embeddings, output2[:, 0, :]), dim=1)\n",
        "\n",
        "        to_classify = self.relu(self.batchnorm_linear(self.linear(pooled_embeddings)))\n",
        "        # to_classify = self.dropout(to_classify)\n",
        "        # to_classify = self.relu(self.linear2(to_classify))\n",
        "        to_classify = self.dropout(to_classify)\n",
        "\n",
        "        out = self.classifier(to_classify)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC7tObDAhLlF",
        "outputId": "152285e2-9ee1-49c0-d170-8dbf023afa4f"
      },
      "id": "YC7tObDAhLlF",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, model,train_dataloader, validation_dataloader, optimizer, loss_function, device, scheduler=None):\n",
        "        self.model = model.to(device)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.validation_dataloader = validation_dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.device = device\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluation_parameters(y_true, y_pred):\n",
        "        #breakpoint()\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        return cm, precision, recall, f1, accuracy\n",
        "\n",
        "    @staticmethod\n",
        "    def format_time_delay(seconds):\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds % 3600) // 60\n",
        "        seconds = seconds % 60\n",
        "        return hours, minutes, seconds\n",
        "\n",
        "\n",
        "    def train(self, epochs: int, use_wandb: bool = False, config: dict = {}, name: str=\"\", target_f1: float=0.0):\n",
        "        start_time = time.time()\n",
        "        best_model = None\n",
        "        old_name = ''\n",
        "        if use_wandb:\n",
        "            wandb.init(\n",
        "                # Set the project where this run will be logged\n",
        "                project=\"nlphw2\",\n",
        "                name=name,\n",
        "                # Track hyperparameters and run metadata\n",
        "                config=config\n",
        "            )\n",
        "        validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n",
        "        total_loss = validation_loss\n",
        "        if use_wandb:\n",
        "                wandb.log({\"validation_loss\": validation_loss,\n",
        "                      \"precision\": precision,\n",
        "                      \"recall\": recall,\n",
        "                      \"f1\": f1,\n",
        "                      \"accuracy\": accuracy,\n",
        "                      \"train_loss\": total_loss / len(self.train_dataloader)})\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            time_delay = time.time() - start_time\n",
        "            hours, minutes, seconds = self.format_time_delay(time_delay)\n",
        "            print(f\"\\nTempo trascorso: {hours} ore, {minutes} minuti, {seconds} secondi\")\n",
        "            self.model.freeze(epoch)\n",
        "            self.model.train()  # Set the model to training mode\n",
        "            total_loss = 0\n",
        "            #breakpoint()\n",
        "            for i, batch in enumerate(self.train_dataloader):\n",
        "                print_progress_bar(i / len(self.train_dataloader), text=f\" | training epoch {epoch}\")\n",
        "                # Get the inputs and targets from the batch\n",
        "                inputs, mask, targets, edges, verbs, classic_inputs = batch\n",
        "\n",
        "                # Zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                # Forward pass\n",
        "                outputs = self.model(inputs, mask,  edges, verbs, classic_inputs)\n",
        "                #print(\"outputs = \", outputs,\"\\ntargets = \", targets)\n",
        "                #breakpoint()\n",
        "                # Compute loss\n",
        "                loss = self.loss_function(outputs, targets)\n",
        "                #print(loss)\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # Accumulate the total loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # Print the average loss for this epoch\n",
        "            validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n",
        "            if f1 > target_f1:\n",
        "                best_model = self.model.state_dict()\n",
        "                target_f1 = f1\n",
        "                if old_name != '':\n",
        "                    os.remove(old_name)\n",
        "                old_name = name + f'-{target_f1}.pth'\n",
        "                torch.save(best_model, name + f'-{target_f1}.pth')\n",
        "            if use_wandb:\n",
        "                wandb.log({\"validation_loss\": validation_loss,\n",
        "                      \"precision\": precision,\n",
        "                      \"recall\": recall,\n",
        "                      \"f1\": f1,\n",
        "                      \"accuracy\": accuracy,\n",
        "                      \"train_loss\": total_loss / len(self.train_dataloader)})\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "        print('\\nbest f1: ', target_f1)\n",
        "        if use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "    def validate(self, use_wandb: bool = False, test_dataloader=None, load_from=''):\n",
        "        if os.path.isfile(load_from):\n",
        "            self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n",
        "        dataloader = self.validation_dataloader if test_dataloader is None else test_dataloader\n",
        "        if dataloader is None:\n",
        "            print(\"empty dataloader!\")\n",
        "            exit(1)\n",
        "        self.model.eval()  # Set the model to evaluation mode\n",
        "        total_loss = 0\n",
        "        all_predictions = torch.tensor([])\n",
        "        all_targets = torch.tensor([])\n",
        "        with torch.no_grad():  # Do not calculate gradients\n",
        "            for i, batch in enumerate(dataloader):\n",
        "                print_progress_bar(i / len(dataloader), text=\" | validation\")\n",
        "                # Get the inputs and targets from the batch\n",
        "                inputs, mask, targets,  edges, verbs, classic_inputs  = batch\n",
        "\n",
        "                # Forward pass\n",
        "                #breakpoint()\n",
        "                outputs = self.model(inputs, mask,  edges, verbs, classic_inputs)\n",
        "                # Compute loss\n",
        "                #breakpoint()\n",
        "                loss = self.loss_function(outputs, targets)\n",
        "                # Accumulate the total loss\n",
        "                total_loss += loss.item()\n",
        "                # Store predictions and targets\n",
        "                all_predictions = torch.cat((all_predictions, outputs.squeeze().round().cpu()))\n",
        "                all_targets = torch.cat((all_targets, targets.cpu()))\n",
        "        validation_loss = total_loss / len(self.validation_dataloader)\n",
        "        #breakpoint()\n",
        "        cm, precision, recall, f1, accuracy = self.evaluation_parameters(all_targets, all_predictions)\n",
        "        return validation_loss, precision, recall, f1, accuracy, cm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6d02JsMlhdCG"
      },
      "id": "6d02JsMlhdCG",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "H5qtdZ12YpKJ",
      "metadata": {
        "id": "H5qtdZ12YpKJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# class NLIDataset(Dataset):\n",
        "\n",
        "\n",
        "\n",
        "#     def __init__(self, data, file_name = '', adversarial = False, do_remove_stopwords = False,\n",
        "#                  do_remove_punctuation = False, do_use_similarities = False, base_set = True, do_lemmatization = False):\n",
        "#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#         self.encode_labels = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
        "#         self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "#         self.model = RobertaModel.from_pretrained('roberta-base').to(self.device)\n",
        "#         self.distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "#         self.file_name = file_name\n",
        "#         self.do_use_similarities = do_use_similarities\n",
        "#         self.do_lemmatization = do_lemmatization\n",
        "#         self.do_remove_punctuation = do_remove_punctuation\n",
        "#         self.base_set = base_set\n",
        "#         self.do_remove_stopwords = do_remove_stopwords\n",
        "#         self.adversarial = adversarial\n",
        "#         self.preprocess_function(data)\n",
        "#         self.tokenizer = None\n",
        "#         self.model = None\n",
        "\n",
        "\n",
        "\n",
        "#     def remove_stopwords(self, sample):\n",
        "#         stop_words = set(stopwords.words('english')) if self.do_remove_stopwords else set()\n",
        "#         sentence = ''\n",
        "#         for data in sample:\n",
        "#             word = data\n",
        "#             is_punct = False\n",
        "#             if self.base_set:\n",
        "#                 is_punct = data['pos'] in ['PUNCT', 'CCONJ', 'DET', 'AUX']\n",
        "#                 word = data['lemma']\n",
        "#             if word not in stop_words and not is_punct:\n",
        "#                 next_word = word\n",
        "#                 if self.do_lemmatization and self.base_set:\n",
        "#                     next_word = data['lemma']\n",
        "#                 elif self.base_set:\n",
        "#                     next_word = data['text']\n",
        "#                 sentence += next_word + ' '\n",
        "#         return sentence.strip()\n",
        "\n",
        "\n",
        "#     def preprocess_function(self, examples):\n",
        "#         premises = []\n",
        "#         answers = []\n",
        "#         hypothesis = []\n",
        "#         ordered_similarities = []\n",
        "#         file_exists = self.file_name != '' and os.path.isfile(\"data/\" + self.file_name)\n",
        "#         if file_exists:\n",
        "#             with open(\"data/\" + self.file_name, \"r\") as f:\n",
        "#                 data_loaded = json.load(f)\n",
        "#             premises = data_loaded[\"premises\"]\n",
        "#             hypothesis = data_loaded[\"hypothesis\"]\n",
        "#             answers = data_loaded[\"answers\"]\n",
        "#             ordered_similarities = data_loaded[\"similarities\"] if \"similarities\" in data_loaded else {}\n",
        "\n",
        "#         # Utilizza un ciclo for per popolare le tre liste\n",
        "#         else:\n",
        "#         #breakpoint()\n",
        "#             for i,example in enumerate(examples):\n",
        "\n",
        "#                 print_progress_bar(i / len(examples), text=\" | preprocessing\")\n",
        "#                 if (self.do_remove_stopwords or self.do_remove_punctuation) and self.base_set:\n",
        "#                     premises.append(self.remove_stopwords(example['wsd'][\"premise\"]))\n",
        "#                     hypothesis.append(self.remove_stopwords(example['wsd'][\"hypothesis\"]))\n",
        "#                 elif self.do_remove_stopwords or self.do_remove_punctuation:\n",
        "#                     continue\n",
        "#                 else:\n",
        "#                     premises.append(example[\"premise\"].strip())\n",
        "#                     hypothesis.append(example[\"hypothesis\"].strip())\n",
        "#                 answers.append(self.encode_labels[example[\"label\"]] )\n",
        "\n",
        "#                 if self.do_use_similarities:\n",
        "#                     s1 = self.embed_sentence(example[\"premise\"].strip())\n",
        "#                     s2 = self.embed_sentence(example[\"hypothesis\"].strip())\n",
        "#                     if self.adversarial:\n",
        "#                         ordered_similarities.append(cosine_similarity(s1, s2).item())\n",
        "#                     else:\n",
        "#                         ordered_similarities.append(cosine_similarity(s1, s2).item())\n",
        "#         #breakpoint()\n",
        "#         inputs = self.distilbert_tokenizer(\n",
        "#             hypothesis,\n",
        "#             premises,\n",
        "#             max_length=384,\n",
        "#             truncation=\"only_second\",\n",
        "#             return_offsets_mapping=True,\n",
        "#             padding=\"max_length\",\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "#         data_to_save = {\"premises\": premises, \"hypothesis\": hypothesis, \"answers\": answers}\n",
        "#         if self.do_use_similarities:\n",
        "#             data_to_save[\"similarities\"] = ordered_similarities\n",
        "#             inputs[\"similarity\"] = torch.tensor(ordered_similarities)\n",
        "#         if not file_exists and self.file_name != '':\n",
        "#             if not os.path.exists(\"data\"):\n",
        "#                 os.makedirs(\"data\")\n",
        "#             with open(\"data/\" + self.file_name, \"w\") as f:\n",
        "#                 json.dump(data_to_save, f, indent=4)\n",
        "#         inputs[\"label\"] = torch.tensor(answers)\n",
        "#         self.data = inputs\n",
        "\n",
        "\n",
        "#     def embed_sentence(self, sentence):\n",
        "#         # Tokenizza la frase\n",
        "#         inputs = self.tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
        "#         # Ottieni gli embedding dal modello\n",
        "#         with torch.no_grad():\n",
        "#             outputs = self.model(**inputs.to(self.device))\n",
        "#         # Usa l'output del modello come embedding (puoi usare altri livelli o combinazioni se preferisci)\n",
        "#         # Prendi il vettore medio (puoi anche scegliere il vettore della [CLS] token, ecc.)\n",
        "#         embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "#         return embeddings\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data[\"input_ids\"])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if \"similarity\" in self.data:\n",
        "#             return self.data[\"input_ids\"][idx], self.data[\"attention_mask\"][idx], self.data[\"label\"][idx], self.data[\"similarity\"][idx]\n",
        "#         else:\n",
        "#             return self.data[\"input_ids\"][idx], self.data[\"attention_mask\"][idx], self.data[\"label\"][idx], torch.zeros(1)\n",
        "\n",
        "#     def collate(self, batch):\n",
        "#         #breakpoint()\n",
        "#         x = []\n",
        "#         attention_mask = []\n",
        "#         y = []\n",
        "#         z = []\n",
        "#         for x_batch, attention_mask_batch, y_batch, z_batch in batch:\n",
        "#             x.append(x_batch)\n",
        "#             attention_mask.append(attention_mask_batch)\n",
        "#             y.append(y_batch)\n",
        "#             z.append(z_batch)\n",
        "\n",
        "#         x = torch.stack(x)\n",
        "#         attention_mask = torch.stack(attention_mask)\n",
        "#         y = torch.stack(y)\n",
        "#         z = torch.stack(z)\n",
        "#         #breakpoint()\n",
        "#         # x = pad_sequence(x, batch_first=True)\n",
        "#         # attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
        "#         return x.to(self.device), attention_mask.to(self.device), y.to(self.device), z.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "#     def get_dataloader(self, batch_size):\n",
        "#         return DataLoader(self, batch_size=batch_size, shuffle=True, collate_fn = self.collate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4wAtQ4CWvtV-",
      "metadata": {
        "id": "4wAtQ4CWvtV-"
      },
      "outputs": [],
      "source": [
        "# # prompt: create a torch model using roberta and a linear layer\n",
        "\n",
        "# class RobertaClassifier(nn.Module):\n",
        "#   def __init__(self, use_similarity, num_labels=3, dropout = 0):\n",
        "#     super(RobertaClassifier, self).__init__()\n",
        "#     self.distilbert =  AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "#     self.linear = nn.Sequential(\n",
        "#         nn.Linear(self.distilbert.config.hidden_size+1, 512),\n",
        "#         nn.BatchNorm1d(512)\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(dropout),\n",
        "#         nn.Linear(512, 128),\n",
        "#         nn.BatchNorm1d(128)\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(dropout),\n",
        "#         nn.Linear(128, 16),\n",
        "#         nn.BatchNorm1d(16)\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(dropout),\n",
        "#         nn.Linear(16, num_labels)\n",
        "#     )\n",
        "\n",
        "#     self.use_similarity = use_similarity\n",
        "#     self.initialize_weights()\n",
        "\n",
        "#   def initialize_weights(self):\n",
        "#     # Funzione di inizializzazione dei pesi\n",
        "#     for layer in self.linear:\n",
        "#       if isinstance(layer, nn.Linear):\n",
        "#           # Inizializzazione dei pesi con una distribuzione uniforme\n",
        "#           nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
        "#           # Inizializzazione dei bias a zero\n",
        "#           if layer.bias is not None:\n",
        "#               nn.init.zeros_(layer.bias)\n",
        "\n",
        "#   def freeze(self, epoch):\n",
        "#     if epoch == 2:\n",
        "#         freeze_until_layer = 99999\n",
        "#     else:\n",
        "#         freeze_until_layer = None  # Non congela nulla\n",
        "\n",
        "#     # Congela i layer di DistilBERT\n",
        "#     if freeze_until_layer is not None:\n",
        "#         for idx, param in enumerate(self.distilbert.parameters()):\n",
        "#             if idx < freeze_until_layer:\n",
        "#                 param.requires_grad = False\n",
        "#             else:\n",
        "#                 param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "#   def forward(self, input_ids, attention_mask, similarities):\n",
        "\n",
        "#    # breakpoint()\n",
        "#     if not self.use_similarity:\n",
        "#         similarities = torch.zeros(input_ids.shape[0]).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#     #breakpoint()\n",
        "\n",
        "#     pooled_output = torch.cat((outputs.last_hidden_state[:, 0, :], similarities.unsqueeze(1)), dim=1)\n",
        "#     # outputs.last_hidden_state.mean(dim=-1)\n",
        "\n",
        "#     logits = self.linear(pooled_output)\n",
        "\n",
        "#     return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# class Trainer():\n",
        "\n",
        "#     def __init__(self, model,train_dataloader, validation_dataloader, optimizer, loss_function, device, scheduler=None):\n",
        "#         self.model = model.to(device)\n",
        "#         self.train_dataloader = train_dataloader\n",
        "#         self.validation_dataloader = validation_dataloader\n",
        "#         self.optimizer = optimizer\n",
        "#         self.loss_function = loss_function\n",
        "#         self.device = device\n",
        "#         self.scheduler = scheduler\n",
        "\n",
        "\n",
        "#     #@staticmethod\n",
        "#     def evaluation_parameters(y_true, y_pred):\n",
        "#         #breakpoint()\n",
        "#         y_pred = np.argmax(y_pred, axis=1)\n",
        "#         cm = confusion_matrix(y_true, y_pred)\n",
        "#         precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#         recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#         f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#         accuracy = accuracy_score(y_true, y_pred)\n",
        "#         return cm, precision, recall, f1, accuracy\n",
        "\n",
        "#     #@staticmethod\n",
        "#     def format_time_delay(seconds):\n",
        "#         hours = seconds // 3600\n",
        "#         minutes = (seconds % 3600) // 60\n",
        "#         seconds = seconds % 60\n",
        "#         return hours, minutes, seconds\n",
        "\n",
        "\n",
        "#     def train(self, epochs: int, use_wandb: bool = False, config: dict = {}, name: str=\"\", target_f1: float=0.0):\n",
        "#         start_time = time.time()\n",
        "#         best_model = None\n",
        "#         old_name = ''\n",
        "#         if use_wandb:\n",
        "#             wandb.init(\n",
        "#                 # Set the project where this run will be logged\n",
        "#                 project=\"nlphw2\",\n",
        "#                 name=name,\n",
        "#                 # Track hyperparameters and run metadata\n",
        "#                 config=config\n",
        "#             )\n",
        "#         validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n",
        "#         total_loss = validation_loss\n",
        "#         if use_wandb:\n",
        "#                 wandb.log({\"validation_loss\": validation_loss,\n",
        "#                       \"precision\": precision,\n",
        "#                       \"recall\": recall,\n",
        "#                       \"f1\": f1,\n",
        "#                       \"accuracy\": accuracy,\n",
        "#                       \"train_loss\": total_loss / len(self.train_dataloader)})\n",
        "#         for epoch in range(epochs):\n",
        "\n",
        "#             time_delay = time.time() - start_time\n",
        "#             hours, minutes, seconds = self.format_time_delay(time_delay)\n",
        "#             print(f\"\\nTempo trascorso: {hours} ore, {minutes} minuti, {seconds} secondi\")\n",
        "#             self.model.freeze(epoch)\n",
        "#             self.model.train()  # Set the model to training mode\n",
        "#             total_loss = 0\n",
        "#             #breakpoint()\n",
        "#             for i, batch in enumerate(self.train_dataloader):\n",
        "#                 print_progress_bar(i / len(self.train_dataloader), text=f\" | training epoch {epoch}\")\n",
        "#                 # Get the inputs and targets from the batch\n",
        "#                 inputs, mask, targets, similarities = batch\n",
        "\n",
        "#                 # Zero the gradients\n",
        "#                 self.optimizer.zero_grad()\n",
        "#                 # Forward pass\n",
        "#                 outputs = self.model(inputs, mask, similarities)\n",
        "#                 #print(\"outputs = \", outputs,\"\\ntargets = \", targets)\n",
        "#                 #breakpoint()\n",
        "#                 # Compute loss\n",
        "#                 loss = self.loss_function(outputs, targets)\n",
        "#                 #print(loss)\n",
        "#                 # Backward pass and optimize\n",
        "#                 loss.backward()\n",
        "#                 self.optimizer.step()\n",
        "#                 # Accumulate the total loss\n",
        "#                 total_loss += loss.item()\n",
        "\n",
        "#             # Print the average loss for this epoch\n",
        "#             validation_loss, precision, recall, f1, accuracy, cm = self.validate(use_wandb)\n",
        "#             if f1 > target_f1:\n",
        "#                 best_model = self.model.state_dict()\n",
        "#                 target_f1 = f1\n",
        "#                 if old_name != '':\n",
        "#                     os.remove(old_name)\n",
        "#                 old_name = name + f'-{target_f1}.pth'\n",
        "#                 torch.save(best_model, name + f'-{target_f1}.pth')\n",
        "#             if use_wandb:\n",
        "#                 wandb.log({\"validation_loss\": validation_loss,\n",
        "#                       \"precision\": precision,\n",
        "#                       \"recall\": recall,\n",
        "#                       \"f1\": f1,\n",
        "#                       \"accuracy\": accuracy,\n",
        "#                       \"train_loss\": total_loss / len(self.train_dataloader)})\n",
        "#             if self.scheduler is not None:\n",
        "#                 self.scheduler.step()\n",
        "#         print('\\nbest f1: ', target_f1)\n",
        "#         if use_wandb:\n",
        "#             wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "#     def validate(self, use_wandb: bool = False, test_dataloader=None, load_from=''):\n",
        "#         if os.path.isfile(load_from):\n",
        "#             self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n",
        "#         dataloader = self.validation_dataloader if test_dataloader is None else test_dataloader\n",
        "#         if dataloader is None:\n",
        "#             print(\"empty dataloader!\")\n",
        "#             exit(1)\n",
        "#         self.model.eval()  # Set the model to evaluation mode\n",
        "#         total_loss = 0\n",
        "#         all_predictions = torch.tensor([])\n",
        "#         all_targets = torch.tensor([])\n",
        "#         with torch.no_grad():  # Do not calculate gradients\n",
        "#             for i, batch in enumerate(dataloader):\n",
        "#                 print_progress_bar(i / len(dataloader), text=\" | validation\")\n",
        "#                 # Get the inputs and targets from the batch\n",
        "#                 inputs, mask, targets, similarities  = batch\n",
        "\n",
        "#                 # Forward pass\n",
        "#                 outputs = self.model(inputs, mask, similarities)\n",
        "#                 # Compute loss\n",
        "#                 #breakpoint()\n",
        "#                 loss = self.loss_function(outputs, targets)\n",
        "#                 # Accumulate the total loss\n",
        "#                 total_loss += loss.item()\n",
        "#                 # Store predictions and targets\n",
        "#                 all_predictions = torch.cat((all_predictions, outputs.squeeze().round().cpu()))\n",
        "#                 all_targets = torch.cat((all_targets, targets.cpu()))\n",
        "#         validation_loss = total_loss / len(self.validation_dataloader)\n",
        "#         #breakpoint()\n",
        "#         cm, precision, recall, f1, accuracy = self.evaluation_parameters(all_targets, all_predictions)\n",
        "#         return validation_loss, precision, recall, f1, accuracy, cm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcK9u6vN8qQf"
      },
      "id": "dcK9u6vN8qQf",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cnqcVF1m7Gx6",
      "metadata": {
        "id": "cnqcVF1m7Gx6"
      },
      "outputs": [],
      "source": [
        "class BaselineStratifiedModel(nn.Module):\n",
        "\n",
        "    def __init__(self, len0, len1):\n",
        "        super(BaselineStratifiedModel, self).__init__()\n",
        "        self.p = len0/(len0+len1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.tensor([0 if np.random.rand() < self.p else 1 for _ in range(x[0].shape[0])], dtype=torch.float)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "uF0IzCm6Rk8O",
      "metadata": {
        "id": "uF0IzCm6Rk8O"
      },
      "outputs": [],
      "source": [
        "new_seed = 108\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    rnd.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True # Se stai usando GPU\n",
        "    return seed, seed+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Tex3y-pe6ttW",
      "metadata": {
        "id": "Tex3y-pe6ttW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Imposta il seed per avere riproducibilità\n",
        "\n",
        "seed, new_seed = set_seed(new_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1MTLav9Ttb7q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MTLav9Ttb7q",
        "outputId": "a5d70f3f-0f3a-4fc7-cf82-75b1ddf3cb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmonteleone-1883922\u001b[0m (\u001b[33mmonteleone\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\n",
        "adversarial = load_dataset(\"iperbole/adversarial_fever_nli\")[\"test\"]\n",
        "\n",
        "ds = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
        "\n",
        "training_set = ds[\"train\"]\n",
        "\n",
        "validation_set = ds[\"validation\"]\n",
        "\n",
        "test_set = ds[\"test\"]\n",
        "\n",
        "wandb.login(key='aaf831dabc88d936d4e6b439b798bb4cb42814ea')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Xx8K6p8vvKBN",
      "metadata": {
        "id": "Xx8K6p8vvKBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f6cea8-cebd-4815-9088-adf4ebfded43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'part': 'manual_adversarial',\n",
              " 'cid': 58846,\n",
              " 'premise': 'Johnny Galecki . He is known for playing David Healy in the ABC sitcom Roseanne from 1992 -- 1997 and Dr. Leonard Hofstadter in the CBS sitcom The Big Bang Theory since 2007 .',\n",
              " 'hypothesis': 'The number of sitcoms from France in which Johnny Galecki has played a character is greater or equal to 2',\n",
              " 'label': 'NEUTRAL'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# !git clone https://github.com/monteleone-1883922/hw2_nlp.git\n",
        "# os.chdir(\"hw2_nlp\")\n",
        "# !git checkout huggingFaceBase\n",
        "# !mv data ./../data\n",
        "# os.chdir(\"..\")\n",
        "# !rm -rf hw2_nlp\n",
        "adversarial[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cONolUaUahmF",
      "metadata": {
        "id": "cONolUaUahmF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9j_yPVcNvUzd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j_yPVcNvUzd",
        "outputId": "542edd15-7219-4a89-a771-a56d9af92e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = NLIDataset(training_set, \"training.json\", load=True)#, do_remove_punctuation=True)\n",
        "\n",
        "validation_dataset = NLIDataset(validation_set, \"validation.json\", load=True)#, do_remove_punctuation=True)\n",
        "\n",
        "test_dataset = NLIDataset(test_set, \"test.json\", load=True)#, do_remove_punctuation=True)\n",
        "\n",
        "#adversarial_dataset = NLIDataset(adversarial, \"erwwwrre4.json\")#, adversarial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "GkXZIxkPvVrW",
      "metadata": {
        "id": "GkXZIxkPvVrW"
      },
      "outputs": [],
      "source": [
        "# prompt: genera i dataloader\n",
        "\n",
        "train_dataloader = train_dataset.get_dataloader(batch_size=20)\n",
        "validation_dataloader = validation_dataset.get_dataloader(batch_size=20)\n",
        "test_dataloader = test_dataset.get_dataloader(batch_size=20)\n",
        "#adversarial_dataloader = adversarial_dataset.get_dataloader(batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "zZdBugO7u5pW",
      "metadata": {
        "id": "zZdBugO7u5pW"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = RobertaClassifier(False, dropout=0.4)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
        "\n",
        "trainer = Trainer(model, train_dataloader, validation_dataloader, optimizer, nn.CrossEntropyLoss(),'cuda' if torch.cuda.is_available() else 'cpu')#, scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[2],0.2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"gnn_adamW_5e-5_108-0.701737717971092.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAdbhXtvCMIg",
        "outputId": "99153a44-0043-4d52-dd48-6e78ec6300a7"
      },
      "id": "bAdbhXtvCMIg",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_19229/2040456891.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"gnn_adamW_5e-5_108-0.701737717971092.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf data"
      ],
      "metadata": {
        "id": "Yot5Bec8KbUA"
      },
      "id": "Yot5Bec8KbUA",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsdKlL-WyzAD"
      },
      "id": "lsdKlL-WyzAD",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "nJk1swaCF70a",
      "metadata": {
        "id": "nJk1swaCF70a"
      },
      "outputs": [],
      "source": [
        "# trainer.validate(test=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASCjcQMQvw9O",
      "metadata": {
        "id": "ASCjcQMQvw9O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "a9d32a6f-0b11-457b-c714-57d197523258"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241019_183740-gnv3vuwp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/monteleone/nlphw2/runs/gnv3vuwp' target=\"_blank\">gnn_adamW_5e-5_108</a></strong> to <a href='https://wandb.ai/monteleone/nlphw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/monteleone/nlphw2' target=\"_blank\">https://wandb.ai/monteleone/nlphw2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/monteleone/nlphw2/runs/gnv3vuwp' target=\"_blank\">https://wandb.ai/monteleone/nlphw2/runs/gnv3vuwp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[============================> ] 99.13% complete  | validation\n",
            "Tempo trascorso: 0.0 ore, 1.0 minuti, 25.765451192855835 secondi\n",
            "[=====================>        ] 73.66% complete  | training epoch 0"
          ]
        }
      ],
      "source": [
        "\n",
        "trainer.train(4, use_wandb=True, name=\"gnn_adamW_5e-5_\"+ str(seed))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-uPr98CKaAk"
      },
      "id": "T-uPr98CKaAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86fceafe-aa0b-4cf5-a81d-68a7bdbd1089",
      "metadata": {
        "tags": [],
        "id": "86fceafe-aa0b-4cf5-a81d-68a7bdbd1089",
        "outputId": "a7bddd5f-aad1-462d-9b8a-29b10f64c8cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_110/1919384173.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[============================> ] 98.61% complete  | validation"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_110/1919384173.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(load_from, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==========================>   ] 90.91% complete  | validation"
          ]
        }
      ],
      "source": [
        "validation_loss_1, precision_1, recall_1, f1_1, accuracy_1, cm_1 = trainer.validate(test_dataloader=test_dataloader, load_from='simil_1e-5_108-0.7299378113069238.pth')\n",
        "\n",
        "validation_loss_2, precision_2, recall_2, f1_2, accuracy_2, cm_2 = trainer.validate(test_dataloader=adversarial_dataloader, load_from='simil_1e-5_108-0.7299378113069238.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc0747c-5d1d-4dc9-bc5e-e47a8753ad9f",
      "metadata": {
        "tags": [],
        "id": "4dc0747c-5d1d-4dc9-bc5e-e47a8753ad9f",
        "outputId": "2ae2dad1-24ca-45e2-93b7-c3bcf72cd15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6916480330296884\n",
            "0.54832453943422\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(f1_1)\n",
        "print(f1_2)\n",
        "print(type(cm_1))\n",
        "\n",
        "# Visualizza la matrice di confusione\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f2992e-43bf-465e-a11a-54d1189b961c",
      "metadata": {
        "tags": [],
        "id": "32f2992e-43bf-465e-a11a-54d1189b961c",
        "outputId": "0c20a69e-5a81-41ff-84d9-574ff470a9ba"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=['CONTRADICTION', 'NEUTRAL','ENTAILMENT' ])\n",
        "disp.plot()\n",
        "\n",
        "# Se stai lavorando in un ambiente interattivo come Jupyter Notebook, la matrice verrà visualizzata automaticamente\n",
        "# Se sei in uno script Python, puoi usare plt.show() per visualizzare la matrice\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63b6999-7500-4d8d-bc09-4287111f4c0a",
      "metadata": {
        "tags": [],
        "id": "b63b6999-7500-4d8d-bc09-4287111f4c0a",
        "outputId": "9f085008-d094-492d-da71-2c8c29e321af"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_2, display_labels=['CONTRADICTION', 'NEUTRAL','ENTAILMENT' ])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3vUyqHDfBiYG",
      "metadata": {
        "id": "3vUyqHDfBiYG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# del training_set\n",
        "\n",
        "# dataset = NLIDataset(validation_set)\n",
        "\n",
        "\n",
        "# dataloader = dataset.get_dataloader(batch_size=32, pos_num=1)\n",
        "\n",
        "# model = RobertaClassifier()\n",
        "\n",
        "# batch = next(iter(dataloader))\n",
        "\n",
        "# model(batch[0], batch[1], batch[3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": ".conda-default:Python",
      "language": "python",
      "name": "conda-env-.conda-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}