{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#INSTALL LIBRARIES------------------------------------------\n",
        "!pip install transformers scikit-learn datasets wandb"
      ],
      "metadata": {
        "id": "nHARZQ6fIXm8",
        "outputId": "0c4400ec-6f12-4cbb-cfb8-d7d6d669fe46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nHARZQ6fIXm8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS-----------------------------\n",
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from torch.nn import Linear, ReLU\n",
        "import pdb\n",
        "import numpy as np, torch, random as rnd, torch.nn as nn, wandb\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "aZ4RHIcXDNcA"
      },
      "id": "aZ4RHIcXDNcA",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "f875cede-e1cb-4961-d5c4-f4fdfd4f49e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cid': 58846,\n",
            " 'hypothesis': 'The number of sitcoms from France in which Johnny Galecki has '\n",
            "               'played a character is greater or equal to 2',\n",
            " 'label': 'NEUTRAL',\n",
            " 'part': 'manual_adversarial',\n",
            " 'premise': 'Johnny Galecki . He is known for playing David Healy in the ABC '\n",
            "            'sitcom Roseanne from 1992 -- 1997 and Dr. Leonard Hofstadter in '\n",
            "            'the CBS sitcom The Big Bang Theory since 2007 .'}\n",
            "{'hypothesis': 'Roman Atwood is a content creator.',\n",
            " 'id': '150448',\n",
            " 'label': 'ENTAILMENT',\n",
            " 'premise': 'Roman Atwood . He is best known for his vlogs , where he posts '\n",
            "            'updates about his life on a daily basis . His vlogging channel , '\n",
            "            \"`` RomanAtwoodVlogs '' , has a total of 3.3 billion views and \"\n",
            "            '11.9 million subscribers . He also has another YouTube channel '\n",
            "            \"called `` RomanAtwood '' , where he posts pranks .\",\n",
            " 'srl': {'hypothesis': {'annotations': [{'englishPropbank': {'frameName': 'be.01',\n",
            "                                                             'roles': [{'role': 'ARG1',\n",
            "                                                                        'score': 1.0,\n",
            "                                                                        'span': [0,\n",
            "                                                                                 2]},\n",
            "                                                                       {'role': 'ARG2',\n",
            "                                                                        'score': 1.0,\n",
            "                                                                        'span': [3,\n",
            "                                                                                 6]}]},\n",
            "                                         'tokenIndex': 2,\n",
            "                                         'verbatlas': {'frameName': 'COPULA',\n",
            "                                                       'roles': [{'role': 'Theme',\n",
            "                                                                  'score': 1.0,\n",
            "                                                                  'span': [0,\n",
            "                                                                           2]},\n",
            "                                                                 {'role': 'Attribute',\n",
            "                                                                  'score': 1.0,\n",
            "                                                                  'span': [3,\n",
            "                                                                           6]}]}}],\n",
            "                        'tokens': [{'index': 0, 'rawText': 'Roman'},\n",
            "                                   {'index': 1, 'rawText': 'Atwood'},\n",
            "                                   {'index': 2, 'rawText': 'is'},\n",
            "                                   {'index': 3, 'rawText': 'a'},\n",
            "                                   {'index': 4, 'rawText': 'content'},\n",
            "                                   {'index': 5, 'rawText': 'creator'},\n",
            "                                   {'index': 6, 'rawText': '.'}]},\n",
            "         'premise': {'annotations': [{'englishPropbank': {'frameName': 'be.01',\n",
            "                                                          'roles': [{'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [3,\n",
            "                                                                              4]},\n",
            "                                                                    {'role': 'ARG2',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [5,\n",
            "                                                                              22]}]},\n",
            "                                      'tokenIndex': 4,\n",
            "                                      'verbatlas': {'frameName': 'COPULA',\n",
            "                                                    'roles': [{'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [3, 4]},\n",
            "                                                              {'role': 'Attribute',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [5,\n",
            "                                                                        22]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'know.01',\n",
            "                                                          'roles': [{'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [3,\n",
            "                                                                              4]},\n",
            "                                                                    {'role': 'ARGM-MNR',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [5,\n",
            "                                                                              6]},\n",
            "                                                                    {'role': 'ARG2',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [7,\n",
            "                                                                              22]}]},\n",
            "                                      'tokenIndex': 6,\n",
            "                                      'verbatlas': {'frameName': 'KNOW',\n",
            "                                                    'roles': [{'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [3, 4]},\n",
            "                                                              {'role': 'Attribute',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [5, 6]},\n",
            "                                                              {'role': 'Topic',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [7,\n",
            "                                                                        22]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'post.01',\n",
            "                                                          'roles': [{'role': 'ARGM-LOC',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [8,\n",
            "                                                                              10]},\n",
            "                                                                    {'role': 'R-ARGM-LOC',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [11,\n",
            "                                                                              12]},\n",
            "                                                                    {'role': 'ARG0',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [12,\n",
            "                                                                              13]},\n",
            "                                                                    {'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [14,\n",
            "                                                                              18]},\n",
            "                                                                    {'role': 'ARGM-TMP',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [18,\n",
            "                                                                              22]}]},\n",
            "                                      'tokenIndex': 13,\n",
            "                                      'verbatlas': {'frameName': 'RECORD',\n",
            "                                                    'roles': [{'role': 'Location',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [8, 10]},\n",
            "                                                              {'role': 'Agent',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [12,\n",
            "                                                                        13]},\n",
            "                                                              {'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [14,\n",
            "                                                                        18]},\n",
            "                                                              {'role': 'Time',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [18,\n",
            "                                                                        22]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'have.03',\n",
            "                                                          'roles': [{'role': 'ARG0',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [23,\n",
            "                                                                              32]},\n",
            "                                                                    {'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [33,\n",
            "                                                                              43]}]},\n",
            "                                      'tokenIndex': 32,\n",
            "                                      'verbatlas': {'frameName': 'EXIST-WITH-FEATURE',\n",
            "                                                    'roles': [{'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [23,\n",
            "                                                                        32]},\n",
            "                                                              {'role': 'Attribute',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [33,\n",
            "                                                                        43]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'have.03',\n",
            "                                                          'roles': [{'role': 'ARG0',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [44,\n",
            "                                                                              45]},\n",
            "                                                                    {'role': 'ARGM-DIS',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [45,\n",
            "                                                                              46]},\n",
            "                                                                    {'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [47,\n",
            "                                                                              60]}]},\n",
            "                                      'tokenIndex': 46,\n",
            "                                      'verbatlas': {'frameName': 'EXIST-WITH-FEATURE',\n",
            "                                                    'roles': [{'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [44,\n",
            "                                                                        45]},\n",
            "                                                              {'role': 'Attribute',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [47,\n",
            "                                                                        60]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'call.01',\n",
            "                                                          'roles': [{'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [47,\n",
            "                                                                              50]},\n",
            "                                                                    {'role': 'ARG2',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [51,\n",
            "                                                                              55]}]},\n",
            "                                      'tokenIndex': 50,\n",
            "                                      'verbatlas': {'frameName': 'NAME',\n",
            "                                                    'roles': [{'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [47,\n",
            "                                                                        50]},\n",
            "                                                              {'role': 'Attribute',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [51,\n",
            "                                                                        55]}]}},\n",
            "                                     {'englishPropbank': {'frameName': 'post.01',\n",
            "                                                          'roles': [{'role': 'ARGM-LOC',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [47,\n",
            "                                                                              55]},\n",
            "                                                                    {'role': 'R-ARGM-LOC',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [56,\n",
            "                                                                              57]},\n",
            "                                                                    {'role': 'ARG0',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [57,\n",
            "                                                                              58]},\n",
            "                                                                    {'role': 'ARG1',\n",
            "                                                                     'score': 1.0,\n",
            "                                                                     'span': [59,\n",
            "                                                                              60]}]},\n",
            "                                      'tokenIndex': 58,\n",
            "                                      'verbatlas': {'frameName': 'RECORD',\n",
            "                                                    'roles': [{'role': 'Location',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [47,\n",
            "                                                                        55]},\n",
            "                                                              {'role': 'Agent',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [57,\n",
            "                                                                        58]},\n",
            "                                                              {'role': 'Theme',\n",
            "                                                               'score': 1.0,\n",
            "                                                               'span': [59,\n",
            "                                                                        60]}]}}],\n",
            "                     'tokens': [{'index': 0, 'rawText': 'Roman'},\n",
            "                                {'index': 1, 'rawText': 'Atwood'},\n",
            "                                {'index': 2, 'rawText': '.'},\n",
            "                                {'index': 3, 'rawText': 'He'},\n",
            "                                {'index': 4, 'rawText': 'is'},\n",
            "                                {'index': 5, 'rawText': 'best'},\n",
            "                                {'index': 6, 'rawText': 'known'},\n",
            "                                {'index': 7, 'rawText': 'for'},\n",
            "                                {'index': 8, 'rawText': 'his'},\n",
            "                                {'index': 9, 'rawText': 'vlogs'},\n",
            "                                {'index': 10, 'rawText': ','},\n",
            "                                {'index': 11, 'rawText': 'where'},\n",
            "                                {'index': 12, 'rawText': 'he'},\n",
            "                                {'index': 13, 'rawText': 'posts'},\n",
            "                                {'index': 14, 'rawText': 'updates'},\n",
            "                                {'index': 15, 'rawText': 'about'},\n",
            "                                {'index': 16, 'rawText': 'his'},\n",
            "                                {'index': 17, 'rawText': 'life'},\n",
            "                                {'index': 18, 'rawText': 'on'},\n",
            "                                {'index': 19, 'rawText': 'a'},\n",
            "                                {'index': 20, 'rawText': 'daily'},\n",
            "                                {'index': 21, 'rawText': 'basis'},\n",
            "                                {'index': 22, 'rawText': '.'},\n",
            "                                {'index': 23, 'rawText': 'His'},\n",
            "                                {'index': 24, 'rawText': 'vlogging'},\n",
            "                                {'index': 25, 'rawText': 'channel'},\n",
            "                                {'index': 26, 'rawText': ','},\n",
            "                                {'index': 27, 'rawText': '`'},\n",
            "                                {'index': 28, 'rawText': '`'},\n",
            "                                {'index': 29, 'rawText': 'RomanAtwoodVlogs'},\n",
            "                                {'index': 30, 'rawText': \"''\"},\n",
            "                                {'index': 31, 'rawText': ','},\n",
            "                                {'index': 32, 'rawText': 'has'},\n",
            "                                {'index': 33, 'rawText': 'a'},\n",
            "                                {'index': 34, 'rawText': 'total'},\n",
            "                                {'index': 35, 'rawText': 'of'},\n",
            "                                {'index': 36, 'rawText': '3.3'},\n",
            "                                {'index': 37, 'rawText': 'billion'},\n",
            "                                {'index': 38, 'rawText': 'views'},\n",
            "                                {'index': 39, 'rawText': 'and'},\n",
            "                                {'index': 40, 'rawText': '11.9'},\n",
            "                                {'index': 41, 'rawText': 'million'},\n",
            "                                {'index': 42, 'rawText': 'subscribers'},\n",
            "                                {'index': 43, 'rawText': '.'},\n",
            "                                {'index': 44, 'rawText': 'He'},\n",
            "                                {'index': 45, 'rawText': 'also'},\n",
            "                                {'index': 46, 'rawText': 'has'},\n",
            "                                {'index': 47, 'rawText': 'another'},\n",
            "                                {'index': 48, 'rawText': 'YouTube'},\n",
            "                                {'index': 49, 'rawText': 'channel'},\n",
            "                                {'index': 50, 'rawText': 'called'},\n",
            "                                {'index': 51, 'rawText': '`'},\n",
            "                                {'index': 52, 'rawText': '`'},\n",
            "                                {'index': 53, 'rawText': 'RomanAtwood'},\n",
            "                                {'index': 54, 'rawText': \"''\"},\n",
            "                                {'index': 55, 'rawText': ','},\n",
            "                                {'index': 56, 'rawText': 'where'},\n",
            "                                {'index': 57, 'rawText': 'he'},\n",
            "                                {'index': 58, 'rawText': 'posts'},\n",
            "                                {'index': 59, 'rawText': 'pranks'},\n",
            "                                {'index': 60, 'rawText': '.'}]}},\n",
            " 'wsd': {'hypothesis': [{'bnSynsetId': 'O',\n",
            "                         'index': 0,\n",
            "                         'lemma': 'Roman',\n",
            "                         'nltkSynset': 'O',\n",
            "                         'pos': 'PROPN',\n",
            "                         'text': 'Roman',\n",
            "                         'wnSynsetOffset': 'O'},\n",
            "                        {'bnSynsetId': 'O',\n",
            "                         'index': 1,\n",
            "                         'lemma': 'Atwood',\n",
            "                         'nltkSynset': 'O',\n",
            "                         'pos': 'PROPN',\n",
            "                         'text': 'Atwood',\n",
            "                         'wnSynsetOffset': 'O'},\n",
            "                        {'bnSynsetId': 'O',\n",
            "                         'index': 2,\n",
            "                         'lemma': 'be',\n",
            "                         'nltkSynset': 'O',\n",
            "                         'pos': 'AUX',\n",
            "                         'text': 'is',\n",
            "                         'wnSynsetOffset': 'O'},\n",
            "                        {'bnSynsetId': 'O',\n",
            "                         'index': 3,\n",
            "                         'lemma': 'a',\n",
            "                         'nltkSynset': 'O',\n",
            "                         'pos': 'DET',\n",
            "                         'text': 'a',\n",
            "                         'wnSynsetOffset': 'O'},\n",
            "                        {'bnSynsetId': 'bn:00100364a',\n",
            "                         'index': 4,\n",
            "                         'lemma': 'content',\n",
            "                         'nltkSynset': 'contented.a.01',\n",
            "                         'pos': 'ADJ',\n",
            "                         'text': 'content',\n",
            "                         'wnSynsetOffset': '588797a'},\n",
            "                        {'bnSynsetId': 'bn:00023660n',\n",
            "                         'index': 5,\n",
            "                         'lemma': 'creator',\n",
            "                         'nltkSynset': 'creator.n.02',\n",
            "                         'pos': 'NOUN',\n",
            "                         'text': 'creator',\n",
            "                         'wnSynsetOffset': '9614315n'},\n",
            "                        {'bnSynsetId': 'O',\n",
            "                         'index': 6,\n",
            "                         'lemma': '.',\n",
            "                         'nltkSynset': 'O',\n",
            "                         'pos': 'PUNCT',\n",
            "                         'text': '.',\n",
            "                         'wnSynsetOffset': 'O'}],\n",
            "         'premise': [{'bnSynsetId': 'bn:00109913a',\n",
            "                      'index': 0,\n",
            "                      'lemma': 'roman',\n",
            "                      'nltkSynset': 'roman.a.01',\n",
            "                      'pos': 'ADJ',\n",
            "                      'text': 'Roman',\n",
            "                      'wnSynsetOffset': '2921569a'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 1,\n",
            "                      'lemma': 'Atwood',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PROPN',\n",
            "                      'text': 'Atwood',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 2,\n",
            "                      'lemma': '.',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '.',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 3,\n",
            "                      'lemma': 'he',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'He',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 4,\n",
            "                      'lemma': 'be',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'AUX',\n",
            "                      'text': 'is',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00117603r',\n",
            "                      'index': 5,\n",
            "                      'lemma': 'well',\n",
            "                      'nltkSynset': 'well.r.02',\n",
            "                      'pos': 'ADV',\n",
            "                      'text': 'best',\n",
            "                      'wnSynsetOffset': '12779r'},\n",
            "                     {'bnSynsetId': 'bn:00090143v',\n",
            "                      'index': 6,\n",
            "                      'lemma': 'know',\n",
            "                      'nltkSynset': 'know.v.04',\n",
            "                      'pos': 'VERB',\n",
            "                      'text': 'known',\n",
            "                      'wnSynsetOffset': '594337v'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 7,\n",
            "                      'lemma': 'for',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'ADP',\n",
            "                      'text': 'for',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 8,\n",
            "                      'lemma': 'his',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'his',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00028604n',\n",
            "                      'index': 9,\n",
            "                      'lemma': 'vlog',\n",
            "                      'nltkSynset': 'play.n.01',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'vlogs',\n",
            "                      'wnSynsetOffset': '7007945n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 10,\n",
            "                      'lemma': ',',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': ',',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 11,\n",
            "                      'lemma': 'where',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'SCONJ',\n",
            "                      'text': 'where',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 12,\n",
            "                      'lemma': 'he',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'he',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00091876v',\n",
            "                      'index': 13,\n",
            "                      'lemma': 'post',\n",
            "                      'nltkSynset': 'post.v.02',\n",
            "                      'pos': 'VERB',\n",
            "                      'text': 'posts',\n",
            "                      'wnSynsetOffset': '991683v'},\n",
            "                     {'bnSynsetId': 'bn:00079238n',\n",
            "                      'index': 14,\n",
            "                      'lemma': 'update',\n",
            "                      'nltkSynset': 'update.n.01',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'updates',\n",
            "                      'wnSynsetOffset': '6643303n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 15,\n",
            "                      'lemma': 'about',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'ADP',\n",
            "                      'text': 'about',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 16,\n",
            "                      'lemma': 'his',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'his',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00051045n',\n",
            "                      'index': 17,\n",
            "                      'lemma': 'life',\n",
            "                      'nltkSynset': 'life.n.01',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'life',\n",
            "                      'wnSynsetOffset': '13963192n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 18,\n",
            "                      'lemma': 'on',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'ADP',\n",
            "                      'text': 'on',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 19,\n",
            "                      'lemma': 'a',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'DET',\n",
            "                      'text': 'a',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00100875a',\n",
            "                      'index': 20,\n",
            "                      'lemma': 'daily',\n",
            "                      'nltkSynset': 'daily.s.01',\n",
            "                      'pos': 'ADJ',\n",
            "                      'text': 'daily',\n",
            "                      'wnSynsetOffset': '1968165a'},\n",
            "                     {'bnSynsetId': 'bn:00008870n',\n",
            "                      'index': 21,\n",
            "                      'lemma': 'basis',\n",
            "                      'nltkSynset': 'footing.n.02',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'basis',\n",
            "                      'wnSynsetOffset': '13790912n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 22,\n",
            "                      'lemma': '.',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '.',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 23,\n",
            "                      'lemma': 'his',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'His',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00090000v',\n",
            "                      'index': 24,\n",
            "                      'lemma': 'vlogge',\n",
            "                      'nltkSynset': 'judge.v.01',\n",
            "                      'pos': 'VERB',\n",
            "                      'text': 'vlogging',\n",
            "                      'wnSynsetOffset': '672277v'},\n",
            "                     {'bnSynsetId': 'bn:00017686n',\n",
            "                      'index': 25,\n",
            "                      'lemma': 'channel',\n",
            "                      'nltkSynset': 'channel.n.07',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'channel',\n",
            "                      'wnSynsetOffset': '3006398n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 26,\n",
            "                      'lemma': ',',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': ',',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 27,\n",
            "                      'lemma': '`',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '`',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 28,\n",
            "                      'lemma': '`',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '`',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 29,\n",
            "                      'lemma': 'romanatwoodvlogs',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'X',\n",
            "                      'text': 'RomanAtwoodVlogs',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 30,\n",
            "                      'lemma': \"''\",\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': \"''\",\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 31,\n",
            "                      'lemma': ',',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': ',',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 32,\n",
            "                      'lemma': 'have',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'AUX',\n",
            "                      'text': 'has',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 33,\n",
            "                      'lemma': 'a',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'DET',\n",
            "                      'text': 'a',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00002008n',\n",
            "                      'index': 34,\n",
            "                      'lemma': 'total',\n",
            "                      'nltkSynset': 'sum.n.05',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'total',\n",
            "                      'wnSynsetOffset': '4353803n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 35,\n",
            "                      'lemma': 'of',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'ADP',\n",
            "                      'text': 'of',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 36,\n",
            "                      'lemma': '3.3',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'NUM',\n",
            "                      'text': '3.3',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 37,\n",
            "                      'lemma': 'billion',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'NUM',\n",
            "                      'text': 'billion',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00071511n',\n",
            "                      'index': 38,\n",
            "                      'lemma': 'view',\n",
            "                      'nltkSynset': 'view.n.03',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'views',\n",
            "                      'wnSynsetOffset': '881649n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 39,\n",
            "                      'lemma': 'and',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'CCONJ',\n",
            "                      'text': 'and',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 40,\n",
            "                      'lemma': '11.9',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'NUM',\n",
            "                      'text': '11.9',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 41,\n",
            "                      'lemma': 'million',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'NUM',\n",
            "                      'text': 'million',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00066366n',\n",
            "                      'index': 42,\n",
            "                      'lemma': 'subscriber',\n",
            "                      'nltkSynset': 'subscriber.n.02',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'subscribers',\n",
            "                      'wnSynsetOffset': '10670483n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 43,\n",
            "                      'lemma': '.',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '.',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 44,\n",
            "                      'lemma': 'he',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'He',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00114246r',\n",
            "                      'index': 45,\n",
            "                      'lemma': 'also',\n",
            "                      'nltkSynset': 'besides.r.02',\n",
            "                      'pos': 'ADV',\n",
            "                      'text': 'also',\n",
            "                      'wnSynsetOffset': '47534r'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 46,\n",
            "                      'lemma': 'have',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'AUX',\n",
            "                      'text': 'has',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 47,\n",
            "                      'lemma': 'another',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'DET',\n",
            "                      'text': 'another',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 48,\n",
            "                      'lemma': 'YouTube',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PROPN',\n",
            "                      'text': 'YouTube',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00015144n',\n",
            "                      'index': 49,\n",
            "                      'lemma': 'channel',\n",
            "                      'nltkSynset': 'duct.n.01',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'channel',\n",
            "                      'wnSynsetOffset': '5250659n'},\n",
            "                     {'bnSynsetId': 'bn:00084385v',\n",
            "                      'index': 50,\n",
            "                      'lemma': 'call',\n",
            "                      'nltkSynset': 'name.v.01',\n",
            "                      'pos': 'VERB',\n",
            "                      'text': 'called',\n",
            "                      'wnSynsetOffset': '1028748v'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 51,\n",
            "                      'lemma': '`',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '`',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 52,\n",
            "                      'lemma': '`',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '`',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 53,\n",
            "                      'lemma': 'RomanAtwood',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PROPN',\n",
            "                      'text': 'RomanAtwood',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 54,\n",
            "                      'lemma': \"''\",\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': \"''\",\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 55,\n",
            "                      'lemma': ',',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': ',',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 56,\n",
            "                      'lemma': 'where',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'SCONJ',\n",
            "                      'text': 'where',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 57,\n",
            "                      'lemma': 'he',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PRON',\n",
            "                      'text': 'he',\n",
            "                      'wnSynsetOffset': 'O'},\n",
            "                     {'bnSynsetId': 'bn:00091876v',\n",
            "                      'index': 58,\n",
            "                      'lemma': 'post',\n",
            "                      'nltkSynset': 'post.v.02',\n",
            "                      'pos': 'VERB',\n",
            "                      'text': 'posts',\n",
            "                      'wnSynsetOffset': '991683v'},\n",
            "                     {'bnSynsetId': 'bn:00004630n',\n",
            "                      'index': 59,\n",
            "                      'lemma': 'prank',\n",
            "                      'nltkSynset': 'antic.n.01',\n",
            "                      'pos': 'NOUN',\n",
            "                      'text': 'pranks',\n",
            "                      'wnSynsetOffset': '427580n'},\n",
            "                     {'bnSynsetId': 'O',\n",
            "                      'index': 60,\n",
            "                      'lemma': '.',\n",
            "                      'nltkSynset': 'O',\n",
            "                      'pos': 'PUNCT',\n",
            "                      'text': '.',\n",
            "                      'wnSynsetOffset': 'O'}]}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "adversarial = load_dataset(\"iperbole/adversarial_fever_nli\")[\"test\"]\n",
        "\n",
        "ds = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
        "\n",
        "training_set = ds[\"train\"]\n",
        "\n",
        "validation_set = ds[\"validation\"]\n",
        "\n",
        "test_set = ds[\"test\"]\n",
        "\n",
        "pprint(adversarial[0])\n",
        "pprint(training_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n",
        "\n"
      ],
      "metadata": {
        "id": "l6yGkP9JJwmn",
        "outputId": "1a905e22-35ab-4b9c-ac4a-0edccaf4d4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l6yGkP9JJwmn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "#model = RobertaModel.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "s7BKrgcuPk45"
      },
      "id": "s7BKrgcuPk45",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = \"test per vedere come va\"\n",
        "f2 = \"questa  una prova\"\n",
        "\n",
        "tokenized = tokenizer(f1+ tokenizer.eos_token + f2, return_tensors='pt', padding='max_length', max_length=40, return_token_type_ids=True)\n",
        "\n",
        "print(tokenized)\n",
        "print(tokenized[\"input_ids\"].shape)\n",
        "print(type(tokenized))\n",
        "\n",
        "out = model(**tokenized)\n",
        "print(out['last_hidden_state'].shape)"
      ],
      "metadata": {
        "id": "izN_nvlqSoUg",
        "outputId": "facb2656-d302-4729-e873-70419423bf6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "izN_nvlqSoUg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 21959,   228,   748,   196,  2816,   283, 13205,     2, 23627,\n",
            "           102,   952, 11423,   542,   102, 11570,   102,     2,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "torch.Size([1, 40])\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "torch.Size([1, 40, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(out.last_hidden_state.mean(dim=-1).squeeze().shape)"
      ],
      "metadata": {
        "id": "X5IohIMiU0Tn",
        "outputId": "ece3ff08-4b18-4247-f206-6005bd6a0a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X5IohIMiU0Tn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLIDataset(Dataset):\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, data):\n",
        "\n",
        "      self.encode_labels = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
        "\n",
        "      self.data = []\n",
        "\n",
        "      for example in data:\n",
        "          #breakpoint()\n",
        "          inputs = tokenizer(example[\"premise\"] + tokenizer.eos_token + example[\"hypothesis\"], return_tensors='pt')\n",
        "          token_ids = inputs['input_ids'][0]\n",
        "          sep_token_id = tokenizer.sep_token_id\n",
        "\n",
        "          # Trovare la posizione del token di separazione\n",
        "          sep_index = (token_ids == sep_token_id).nonzero(as_tuple=True)[0].tolist()[0]\n",
        "\n",
        "          element = {\n",
        "              \"input\": inputs,\n",
        "              \"label\": self.encode_labels[example[\"label\"]],\n",
        "              \"separator_index\": sep_index\n",
        "              }\n",
        "          self.data.append(element)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def collate(self, batch):\n",
        "        x = []\n",
        "        y = []\n",
        "        z = []\n",
        "        attention_mask = []\n",
        "        #print(batch)\n",
        "        #breakpoint()\n",
        "        # Iterare sull'array di dizionari una sola volta\n",
        "        for d in batch:\n",
        "            x.append(d['input']['input_ids'].squeeze())\n",
        "            y.append(d['label'])\n",
        "            z.append(d['separator_index'])\n",
        "            attention_mask.append(d['input']['attention_mask'].squeeze())\n",
        "       #breakpoint()\n",
        "        # Convertire le liste di valori in tensori PyTorch\n",
        "        x = pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "        y = torch.tensor(y)\n",
        "        z = torch.tensor(z)\n",
        "        return x,attention_mask, y, z\n",
        "\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size, pos_num):\n",
        "        self.pos_num = pos_num\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=True, collate_fn = self.collate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H5qtdZ12YpKJ"
      },
      "id": "H5qtdZ12YpKJ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a torch model using roberta and a linear layer\n",
        "\n",
        "class RobertaClassifier(nn.Module):\n",
        "  def __init__(self, num_labels=3):\n",
        "    super(RobertaClassifier, self).__init__()\n",
        "    self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "    self.roberta_similarity = RobertaModel.from_pretrained('roberta-base')\n",
        "    self.linear = nn.Linear(self.roberta.config.hidden_size+1, num_labels)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, separators):\n",
        "\n",
        "    breakpoint()\n",
        "    with torch.no_grad():\n",
        "      outputs = self.roberta_similarity(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      premise = outputs.last_hidden_state[:,0, :]\n",
        "      hypotehsis = outputs.last_hidden_state[torch.arange(outputs.last_hidden_state.size(0)),separators, :]\n",
        "\n",
        "      similarities = cosine_similarity(premise, hypotehsis, dim=-1)\n",
        "      del premise, hypotehsis\n",
        "      #breakpoint()\n",
        "\n",
        "    outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    del outputs\n",
        "    pooled_output = torch.cat((pooled_output, similarities.unsqueeze(dim=1)), dim=1)\n",
        "    # outputs.last_hidden_state.mean(dim=-1)\n",
        "    logits = self.linear(pooled_output)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "4wAtQ4CWvtV-"
      },
      "id": "4wAtQ4CWvtV-",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print a progress bar\n",
        "def print_progress_bar(percentuale: float, lunghezza_barra: int = 30, text: str=\"\") -> None:\n",
        "    blocchi_compilati = int(lunghezza_barra * percentuale)\n",
        "    barra = \"[\" + \"=\" * (blocchi_compilati - 1) + \">\" + \" \" * (lunghezza_barra - blocchi_compilati) + \"]\"\n",
        "    sys.stdout.write(f\"\\r{barra} {percentuale * 100:.2f}% complete \" + text)\n",
        "    sys.stdout.flush()"
      ],
      "metadata": {
        "id": "yUEyqUjxq8tC"
      },
      "id": "yUEyqUjxq8tC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, model,train_dataloader, validation_dataloader, optimizer, loss_function, device, test_dataloader=None):\n",
        "        self.model = model.to(device)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.validation_dataloader = validation_dataloader\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.device = device\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluation_parameters(y_true, y_pred):\n",
        "        cm = confusion_matrix(y_true, y_pred).ravel()\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        return cm, precision, recall, f1, accuracy\n",
        "\n",
        "\n",
        "    def train(self, epochs: int, use_wandb: bool = False, config: dict = {}, name: str=\"\", target_f1: float=0.0, freezing=[]):\n",
        "        best_model = None\n",
        "        save = False\n",
        "        if use_wandb:\n",
        "            wandb.init(\n",
        "                # Set the project where this run will be logged\n",
        "                project=\"nlp-hw-1b\",\n",
        "                name=name,\n",
        "                # Track hyperparameters and run metadata\n",
        "                config=config\n",
        "            )\n",
        "        for epoch in range(epochs):\n",
        "            if epoch in freezing:\n",
        "                self.model.freeze(epoch)\n",
        "            self.model.train()  # Set the model to training mode\n",
        "            total_loss = 0\n",
        "            #breakpoint()\n",
        "            for i, batch in enumerate(self.train_dataloader):\n",
        "                print_progress_bar(i / len(self.train_dataloader), text=f\" | training epoch {epoch}\")\n",
        "                # Get the inputs and targets from the batch\n",
        "                inputs, targets, lens = batch\n",
        "\n",
        "                # Zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                # Forward pass\n",
        "                outputs = self.model((inputs, lens))\n",
        "                #print(\"outputs = \", outputs,\"\\ntargets = \", targets)\n",
        "                #breakpoint()\n",
        "                # Compute loss\n",
        "                loss = self.loss_function(outputs, targets)\n",
        "                #print(loss)\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # Accumulate the total loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # Print the average loss for this epoch\n",
        "            validation_loss, precision, recall, f1, accuracy = self.validate(use_wandb)\n",
        "            if f1 > target_f1:\n",
        "                best_model = self.model.state_dict()\n",
        "                target_f1 = f1\n",
        "                save = True\n",
        "            if use_wandb:\n",
        "                wandb.log({\"validation_loss\": validation_loss,\n",
        "                      \"precision\": precision,\n",
        "                      \"recall\": recall,\n",
        "                      \"f1\": f1,\n",
        "                      \"accuracy\": accuracy,\n",
        "                      \"train_loss\": total_loss / len(self.train_dataloader)})\n",
        "        if save:\n",
        "            torch.save(best_model, name + f'-{target_f1}.pth')\n",
        "        print(target_f1)\n",
        "        if use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "    def validate(self, use_wandb: bool = False, test=False):\n",
        "        dataloader = self.test_dataloader if test else self.validation_dataloader\n",
        "        if dataloader is None:\n",
        "            print(\"empty dataloader!\")\n",
        "            exit(1)\n",
        "        self.model.eval()  # Set the model to evaluation mode\n",
        "        total_loss = 0\n",
        "        all_predictions = torch.tensor([])\n",
        "        all_targets = torch.tensor([])\n",
        "        with torch.no_grad():  # Do not calculate gradients\n",
        "            for i, batch in enumerate(self.validation_dataloader):\n",
        "                print_progress_bar(i / len(dataloader), text=\" | validation\")\n",
        "                # Get the inputs and targets from the batch\n",
        "                inputs, targets, lens = batch\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model((inputs,lens))\n",
        "                # Compute loss\n",
        "                #breakpoint()\n",
        "                loss = self.loss_function(outputs, targets)\n",
        "                # Accumulate the total loss\n",
        "                total_loss += loss.item()\n",
        "                # Store predictions and targets\n",
        "                all_predictions = torch.cat((all_predictions, outputs.squeeze().round().cpu()))\n",
        "                all_targets = torch.cat((all_targets, targets.cpu()))\n",
        "        validation_loss = total_loss / len(self.validation_dataloader)\n",
        "        #breakpoint()\n",
        "        precision = precision_score(all_targets, all_predictions)\n",
        "        recall = recall_score(all_targets, all_predictions)\n",
        "        f1 = f1_score(all_targets, all_predictions)\n",
        "        accuracy = accuracy_score(all_targets, all_predictions)\n",
        "        return validation_loss, precision, recall, f1, accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T98THho1q4wT"
      },
      "id": "T98THho1q4wT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del training_set\n",
        "\n",
        "dataset = NLIDataset(validation_set)\n",
        "\n",
        "\n",
        "dataloader = dataset.get_dataloader(batch_size=32, pos_num=1)\n",
        "\n",
        "model = RobertaClassifier()\n",
        "\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "model(batch[0], batch[1], batch[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vUyqHDfBiYG",
        "outputId": "589785a8-7973-415f-b6f2-3cd1e240547b"
      },
      "id": "3vUyqHDfBiYG",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(13)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     12 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m      \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     15 \u001b[0;31m      \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(14)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     12 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m      \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     15 \u001b[0;31m      \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     16 \u001b[0;31m      \u001b[0mhypotehsis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(15)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m      \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m      \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     16 \u001b[0;31m      \u001b[0mhypotehsis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(16)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     14 \u001b[0;31m      \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     15 \u001b[0;31m      \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m      \u001b[0mhypotehsis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     18 \u001b[0;31m      \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotehsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(18)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     16 \u001b[0;31m      \u001b[0mhypotehsis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m      \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotehsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     19 \u001b[0;31m      \u001b[0;32mdel\u001b[0m \u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotehsis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     20 \u001b[0;31m      \u001b[0;31m#breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(19)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     18 \u001b[0;31m      \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotehsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 19 \u001b[0;31m      \u001b[0;32mdel\u001b[0m \u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotehsis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     20 \u001b[0;31m      \u001b[0;31m#breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     21 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(13)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     12 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m      \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     15 \u001b[0;31m      \u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(22)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     20 \u001b[0;31m      \u001b[0;31m#breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     21 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m    \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(23)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     21 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     22 \u001b[0;31m    \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 23 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(24)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     22 \u001b[0;31m    \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;31m# outputs.last_hidden_state.mean(dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(25)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     23 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 25 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;31m# outputs.last_hidden_state.mean(dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     27 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(27)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;31m# outputs.last_hidden_state.mean(dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     28 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> poole_output.shape\n",
            "*** NameError: name 'poole_output' is not defined\n",
            "ipdb> pooled_output.shape\n",
            "torch.Size([32, 769])\n",
            "ipdb> n\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x769 and 768x3)\n",
            "> \u001b[0;32m<ipython-input-5-fac945463f68>\u001b[0m(27)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     24 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;31m# outputs.last_hidden_state.mean(dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     28 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 361, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}